"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ª–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SOTA –º–µ—Ç–æ–¥–æ–≤:
- RetinaFace –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è (5 –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫)
- InsightFace/ArcFace –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (iresnet100)
- TTA (Test-Time Augmentation): –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π flip
- –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ-–≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã
- k-reciprocal re-ranking –¥–ª—è –≥—Ä–∞—Ñ–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
- Spectral Clustering –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
- –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
"""

import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
import time

# –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü
try:
    from retinaface import RetinaFace
    RETINAFACE_AVAILABLE = True
except ImportError:
    RETINAFACE_AVAILABLE = False
    print("‚ö†Ô∏è RetinaFace –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")

try:
    from insightface.app import FaceAnalysis
    from insightface.model_zoo import get_model
    INSIGHTFACE_AVAILABLE = True
except ImportError:
    INSIGHTFACE_AVAILABLE = False
    print("‚ö†Ô∏è InsightFace –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
try:
    from facenet_pytorch import MTCNN, InceptionResnetV1
    FACENET_AVAILABLE = True
except ImportError:
    FACENET_AVAILABLE = False
    print("‚ö†Ô∏è FaceNet-PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.cluster import SpectralClustering, AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity, cosine_distances
from sklearn.preprocessing import normalize
from scipy.spatial.distance import cdist

IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS

def _win_long(path: Path) -> str:
    """Windows long path support"""
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p

def imread_safe(path: Path):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Unicode –ø—É—Ç–µ–π"""
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        img = cv2.imdecode(data, cv2.IMREAD_COLOR)
        if img is None:
            return None
        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path.name}: {e}")
        return None

def calculate_blur_score(image: np.ndarray) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º—ã—Ç–∏—è —á–µ—Ä–µ–∑ Variance of Laplacian.
    –ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —Ç–µ–º —á–µ—Ç—á–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image
    
    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
    return laplacian_var

def calculate_face_quality(face_img: np.ndarray, bbox: tuple = None) -> float:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞.
    –£—á–∏—Ç—ã–≤–∞–µ—Ç: —Ä–∞–∑–º–µ—Ä –ª–∏—Ü–∞, —Ä–∞–∑–º—ã—Ç–∏–µ, —è—Ä–∫–æ—Å—Ç—å.
    
    Returns:
        quality_score: 0.0 - 1.0, –≥–¥–µ 1.0 - –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    """
    scores = []
    
    # 1. –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
    if bbox is not None:
        x1, y1, x2, y2 = bbox[:4]
        face_area = (x2 - x1) * (y2 - y1)
        size_score = min(face_area / (200 * 200), 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 200x200
        scores.append(size_score * 0.3)  # 30% –≤–µ—Å–∞
    
    # 2. –û—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Variance of Laplacian
    blur_score = calculate_blur_score(face_img)
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: blur < 100 = –ø–ª–æ—Ö–æ, > 500 = –æ—Ç–ª–∏—á–Ω–æ
    normalized_blur = min(max(blur_score, 100), 500) / 500
    scores.append(normalized_blur * 0.5)  # 50% –≤–µ—Å–∞
    
    # 3. –Ø—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
    if len(face_img.shape) == 3:
        gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)
    else:
        gray = face_img
    
    mean_brightness = np.mean(gray) / 255.0
    # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å: 0.3 - 0.7
    brightness_score = 1.0 - abs(mean_brightness - 0.5) * 2
    scores.append(brightness_score * 0.2)  # 20% –≤–µ—Å–∞
    
    return sum(scores)

def align_face_5points(img: np.ndarray, landmarks: np.ndarray, target_size=(112, 112)):
    """
    –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ª–∏—Ü–∞ –ø–æ 5 –∫–ª—é—á–µ–≤—ã–º —Ç–æ—á–∫–∞–º (–≥–ª–∞–∑–∞, –Ω–æ—Å, —É–≥–ª—ã —Ä—Ç–∞).
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –¥–ª—è ArcFace/InsightFace.
    """
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è 112x112
    src = np.array([
        [38.2946, 51.6963],  # –õ–µ–≤—ã–π –≥–ª–∞–∑
        [73.5318, 51.5014],  # –ü—Ä–∞–≤—ã–π –≥–ª–∞–∑
        [56.0252, 71.7366],  # –ù–æ—Å
        [41.5493, 92.3655],  # –õ–µ–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞
        [70.7299, 92.2041]   # –ü—Ä–∞–≤—ã–π —É–≥–æ–ª —Ä—Ç–∞
    ], dtype=np.float32)
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    if target_size != (112, 112):
        scale = target_size[0] / 112.0
        src = src * scale
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Affine
    dst = landmarks.astype(np.float32)
    tform = cv2.estimateAffinePartial2D(dst, src)[0]
    
    if tform is None:
        # Fallback: –ø—Ä–æ—Å—Ç–æ –∏–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä
        return cv2.resize(img, target_size)
    
    aligned = cv2.warpAffine(img, tform, target_size, flags=cv2.INTER_LINEAR)
    return aligned

class AdvancedFaceRecognition:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü —Å:
    - RetinaFace –¥–µ—Ç–µ–∫—Ü–∏–µ–π
    - InsightFace/ArcFace —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    - TTA –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ-–≤–∑–≤–µ—à–µ–Ω–Ω—ã–º–∏ —à–∞–±–ª–æ–Ω–∞–º–∏
    """
    
    def __init__(self, use_gpu=False, min_face_size=20, confidence_threshold=0.9):
        self.min_face_size = min_face_size
        self.confidence_threshold = confidence_threshold
        self.use_gpu = use_gpu
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        if INSIGHTFACE_AVAILABLE:
            try:
                print("üîß –ó–∞–≥—Ä—É–∂–∞–µ–º InsightFace –º–æ–¥–µ–ª—å (buffalo_l)...")
                self.face_app = FaceAnalysis(
                    name='buffalo_l',
                    providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu 
                             else ['CPUExecutionProvider']
                )
                self.face_app.prepare(ctx_id=0 if use_gpu else -1, det_size=(640, 640))
                self.detector_type = 'insightface'
                print("‚úÖ InsightFace –∑–∞–≥—Ä—É–∂–µ–Ω (buffalo_l —Å ArcFace)")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ InsightFace: {e}")
                self.detector_type = 'none'
                self.face_app = None
        else:
            self.detector_type = 'none'
            self.face_app = None
    
    def detect_and_extract(self, img: np.ndarray, apply_tta=True) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å TTA.
        
        Returns:
            List of dicts with keys: bbox, landmarks, embedding, quality
        """
        if self.detector_type == 'insightface':
            return self._detect_with_insightface(img, apply_tta)
        else:
            return []
    
    def _detect_with_insightface(self, img: np.ndarray, apply_tta=True) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å InsightFace"""
        results = []
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        faces = self.face_app.get(img)
        
        for face in faces:
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ confidence
            if hasattr(face, 'det_score') and face.det_score < self.confidence_threshold:
                continue
            
            bbox = face.bbox.astype(int)
            x1, y1, x2, y2 = bbox
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É
            if (x2 - x1) < self.min_face_size or (y2 - y1) < self.min_face_size:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏—Ü–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            face_img = img[y1:y2, x1:x2]
            if face_img.size == 0:
                continue
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            quality = calculate_face_quality(face_img, bbox)
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (—É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –≤ InsightFace)
            embedding = face.normed_embedding
            
            # TTA: –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π flip
            if apply_tta:
                img_flipped = cv2.flip(img, 1)
                faces_flipped = self.face_app.get(img_flipped)
                
                if len(faces_flipped) > 0:
                    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ª–∏—Ü–æ (–ø–æ –ø–æ–∑–∏—Ü–∏–∏)
                    img_width = img.shape[1]
                    flipped_x1 = img_width - x2
                    flipped_x2 = img_width - x1
                    
                    best_match = None
                    best_iou = 0
                    
                    for f_face in faces_flipped:
                        fx1, fy1, fx2, fy2 = f_face.bbox.astype(int)
                        # –í—ã—á–∏—Å–ª—è–µ–º IoU
                        inter_x1 = max(flipped_x1, fx1)
                        inter_y1 = max(y1, fy1)
                        inter_x2 = min(flipped_x2, fx2)
                        inter_y2 = min(y2, fy2)
                        
                        if inter_x2 > inter_x1 and inter_y2 > inter_y1:
                            inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                            bbox1_area = (flipped_x2 - flipped_x1) * (y2 - y1)
                            bbox2_area = (fx2 - fx1) * (fy2 - fy1)
                            iou = inter_area / (bbox1_area + bbox2_area - inter_area)
                            
                            if iou > best_iou:
                                best_iou = iou
                                best_match = f_face
                    
                    # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                    if best_match is not None and best_iou > 0.5:
                        flipped_embedding = best_match.normed_embedding
                        # –°—Ä–µ–¥–Ω–µ–µ –¥–≤—É—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ re-normalize
                        embedding = (embedding + flipped_embedding) / 2.0
                        embedding = embedding / np.linalg.norm(embedding)
            
            results.append({
                'bbox': bbox,
                'landmarks': face.kps if hasattr(face, 'kps') else None,
                'embedding': embedding,
                'quality': quality,
                'confidence': face.det_score if hasattr(face, 'det_score') else 1.0
            })
        
        return results

def k_reciprocal_rerank(similarity_matrix: np.ndarray, k: int = 3) -> np.ndarray:
    """
    K-reciprocal re-ranking –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞.
    –ü–æ–≤—ã—à–∞–µ—Ç –≤–µ—Å–∞ –¥–ª—è –≤–∑–∞–∏–º–Ω—ã—Ö k-–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.
    
    Args:
        similarity_matrix: –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ (N x N)
        k: –ß–∏—Å–ª–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    
    Returns:
        –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
    """
    N = similarity_matrix.shape[0]
    reranked = similarity_matrix.copy()
    
    # –ù–∞—Ö–æ–¥–∏–º k –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
    # (—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞)
    nearest_neighbors = np.argsort(-similarity_matrix, axis=1)[:, 1:k+1]
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–∑–∞–∏–º–Ω–æ—Å—Ç—å
    for i in range(N):
        for j in range(i + 1, N):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ j –≤ k-NN –¥–ª—è i –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
            i_in_j_neighbors = i in nearest_neighbors[j]
            j_in_i_neighbors = j in nearest_neighbors[i]
            
            if i_in_j_neighbors and j_in_i_neighbors:
                # –£—Å–∏–ª–∏–≤–∞–µ–º —Å–≤—è–∑—å –¥–ª—è –≤–∑–∞–∏–º–Ω—ã—Ö —Å–æ—Å–µ–¥–µ–π
                boost = 1.2
                reranked[i, j] *= boost
                reranked[j, i] *= boost
            elif i_in_j_neighbors or j_in_i_neighbors:
                # –ù–µ–±–æ–ª—å—à–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Å–æ—Å–µ–¥–µ–π
                boost = 1.1
                reranked[i, j] *= boost
                reranked[j, i] *= boost
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –∫ [0, 1]
    reranked = np.clip(reranked, 0, 1)
    
    return reranked

def spectral_clustering_with_validation(
    embeddings: List[np.ndarray],
    n_clusters: int = None,
    quality_weights: List[float] = None,
    k_reciprocal: int = 3,
    verification_threshold: float = 0.35
) -> np.ndarray:
    """
    Spectral Clustering —Å k-reciprocal re-ranking –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π.
    
    Args:
        embeddings: L2-–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        n_clusters: –ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–µ—Å–ª–∏ None - –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        quality_weights: –í–µ—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        k_reciprocal: k –¥–ª—è re-ranking
        verification_threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (cosine distance)
    
    Returns:
        labels: –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """
    X = np.vstack(embeddings)
    N = len(embeddings)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    if quality_weights is not None:
        X_weighted = X * np.array(quality_weights)[:, np.newaxis]
        X_weighted = normalize(X_weighted, norm='l2')
    else:
        X_weighted = X
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞ (–∫–æ—Å–∏–Ω—É—Å–Ω–∞—è)
    similarity = cosine_similarity(X_weighted)
    
    # K-reciprocal re-ranking
    if k_reciprocal > 0:
        similarity = k_reciprocal_rerank(similarity, k=k_reciprocal)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞—Ñ—Ñ–∏–Ω–∏—Ç–∏-–º–∞—Ç—Ä–∏—Ü—É
    affinity = np.maximum(similarity, 0)
    np.fill_diagonal(affinity, 0)  # –û–±–Ω—É–ª—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ
    if n_clusters is None:
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º eigenvalue gap
        from scipy.linalg import eigh
        
        # –í—ã—á–∏—Å–ª—è–µ–º Laplacian
        D = np.diag(affinity.sum(axis=1))
        L = D - affinity
        
        # –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        eigenvalues, _ = eigh(L, D)
        eigenvalues = np.sort(eigenvalues)
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª—å—à–∏–π gap
        gaps = np.diff(eigenvalues)
        n_clusters = np.argmax(gaps[:min(10, len(gaps))]) + 2  # +2 –ø–æ—Ç–æ–º—É —á—Ç–æ –∏–Ω–¥–µ–∫—Å —Å 0
        n_clusters = max(2, min(n_clusters, N // 2))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        
        print(f"üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
    
    # Spectral Clustering
    clustering = SpectralClustering(
        n_clusters=n_clusters,
        affinity='precomputed',
        assign_labels='kmeans',
        random_state=42
    )
    
    labels = clustering.fit_predict(affinity)
    
    # –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    validated_labels = labels.copy()
    
    for cluster_id in range(n_clusters):
        mask = labels == cluster_id
        cluster_embeddings = X[mask]
        
        if len(cluster_embeddings) < 2:
            continue
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥
        centroid = np.mean(cluster_embeddings, axis=0)
        centroid = centroid / np.linalg.norm(centroid)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞
        indices = np.where(mask)[0]
        for idx in indices:
            distance = 1 - np.dot(X[idx], centroid)  # Cosine distance
            
            if distance > verification_threshold:
                # Outlier - –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞–µ–º –≤ –±–ª–∏–∂–∞–π—à–∏–π –≤–∞–ª–∏–¥–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                best_cluster = -1
                best_distance = float('inf')
                
                for other_cluster_id in range(n_clusters):
                    if other_cluster_id == cluster_id:
                        continue
                    
                    other_mask = labels == other_cluster_id
                    other_embeddings = X[other_mask]
                    
                    if len(other_embeddings) == 0:
                        continue
                    
                    other_centroid = np.mean(other_embeddings, axis=0)
                    other_centroid = other_centroid / np.linalg.norm(other_centroid)
                    
                    other_distance = 1 - np.dot(X[idx], other_centroid)
                    
                    if other_distance < best_distance and other_distance < verification_threshold:
                        best_distance = other_distance
                        best_cluster = other_cluster_id
                
                if best_cluster != -1:
                    validated_labels[idx] = best_cluster
                    print(f"  üîÑ –ü–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–µ–Ω outlier –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id} –≤ {best_cluster}")
                else:
                    # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ noise
                    validated_labels[idx] = -1
                    print(f"  ‚ùå Outlier –Ω–µ –ø–æ–¥–æ—à–µ–ª –Ω–∏ –∫ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    for cluster_id in range(n_clusters):
        mask = validated_labels == cluster_id
        cluster_size = np.sum(mask)
        
        if cluster_size == 1:
            idx = np.where(mask)[0][0]
            # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä
            best_cluster = -1
            best_similarity = -1
            
            for other_id in range(n_clusters):
                if other_id == cluster_id:
                    continue
                
                other_mask = validated_labels == other_id
                if np.sum(other_mask) == 0:
                    continue
                
                other_centroid = np.mean(X[other_mask], axis=0)
                other_centroid = other_centroid / np.linalg.norm(other_centroid)
                
                sim = np.dot(X[idx], other_centroid)
                
                if sim > best_similarity and (1 - sim) < verification_threshold:
                    best_similarity = sim
                    best_cluster = other_id
            
            if best_cluster != -1:
                validated_labels[idx] = best_cluster
                print(f"  üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω –æ–¥–∏–Ω–æ—á–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} ‚Üí {best_cluster}")
    
    return validated_labels

def build_plan_advanced(
    input_dir: Path,
    min_face_confidence: float = 0.9,
    min_blur_threshold: float = 100.0,
    n_clusters: int = None,
    apply_tta: bool = True,
    use_gpu: bool = False,
    progress_callback=None,
    include_excluded: bool = False
) -> Dict:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å SOTA –º–µ—Ç–æ–¥–∞–º–∏.
    
    Args:
        input_dir: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        min_face_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
        min_blur_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–∑–∫–æ—Å—Ç–∏
        n_clusters: –ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        apply_tta: –ü—Ä–∏–º–µ–Ω—è—Ç—å Test-Time Augmentation
        use_gpu: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
        progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        include_excluded: –í–∫–ª—é—á–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã–µ –ø–∞–ø–∫–∏
    
    Returns:
        dict —Å clusters, plan, unreadable, no_faces
    """
    print(f"üöÄ [ADVANCED] –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {input_dir}")
    
    input_dir = Path(input_dir)
    start_time = time.time()
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    
    if include_excluded:
        all_images = [p for p in input_dir.rglob("*") if is_image(p)]
    else:
        all_images = [
            p for p in input_dir.rglob("*")
            if is_image(p) and not any(ex in str(p).lower() for ex in excluded_names)
        ]
    
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    if progress_callback:
        progress_callback(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 5)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    recognizer = AdvancedFaceRecognition(
        use_gpu=use_gpu,
        confidence_threshold=min_face_confidence
    )
    
    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑...", 10)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    all_embeddings = []
    all_qualities = []
    owners = []
    img_face_count = {}
    unreadable = []
    no_faces = []
    
    total = len(all_images)
    
    for i, img_path in enumerate(all_images):
        if progress_callback and i % 5 == 0:
            percent = 10 + int((i + 1) / max(total, 1) * 70)
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑: {percent}% ({i+1}/{total})", percent)
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
        img = imread_safe(img_path)
        if img is None:
            unreadable.append(img_path)
            continue
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        try:
            faces = recognizer.detect_and_extract(img, apply_tta=apply_tta)
            
            if not faces:
                no_faces.append(img_path)
                continue
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            valid_faces = []
            for face in faces:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏
                if face['quality'] < 0.3:  # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                    print(f"  ‚ö†Ô∏è –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ª–∏—Ü–∞ –≤ {img_path.name}: {face['quality']:.3f}")
                    continue
                
                valid_faces.append(face)
            
            if not valid_faces:
                no_faces.append(img_path)
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            img_face_count[img_path] = len(valid_faces)
            
            for face in valid_faces:
                all_embeddings.append(face['embedding'])
                all_qualities.append(face['quality'])
                owners.append(img_path)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {img_path.name}: {e}")
            unreadable.append(img_path)
    
    if not all_embeddings:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {len(set(owners))} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    if progress_callback:
        progress_callback(f"üîÑ Spectral Clustering {len(all_embeddings)} –ª–∏—Ü...", 85)
    
    labels = spectral_clustering_with_validation(
        embeddings=all_embeddings,
        n_clusters=n_clusters,
        quality_weights=all_qualities,
        k_reciprocal=3,
        verification_threshold=0.35
    )
    
    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(set(labels))} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    cluster_map = defaultdict(set)
    cluster_by_img = defaultdict(set)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ noise (-1)
    max_label = max(labels) if len(labels) > 0 and max(labels) >= 0 else -1
    next_single_label = max_label + 1
    
    for idx, (label, path) in enumerate(zip(labels, owners)):
        if label == -1:
            unique_label = next_single_label
            cluster_map[unique_label].add(path)
            cluster_by_img[path].add(unique_label)
            next_single_label += 1
        else:
            cluster_map[label].add(path)
            cluster_by_img[path].add(label)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–ª–∞–Ω
    plan = []
    for path in all_images:
        clusters = cluster_by_img.get(path)
        if not clusters:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(clusters)),
            "faces": img_face_count.get(path, 0)
        })
    
    processing_time = time.time() - start_time
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.1f}—Å")
    
    if progress_callback:
        progress_callback(f"‚úÖ –ì–æ—Ç–æ–≤–æ! {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 100)
    
    return {
        "clusters": {
            int(k): [str(p) for p in sorted(v, key=lambda x: str(x))]
            for k, v in cluster_map.items()
        },
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º distribute_to_folders –∏–∑ cluster.py –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
from cluster import distribute_to_folders, process_group_folder

