import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict

# Optional libs with graceful fallbacks
try:
    import faiss  # faiss-cpu
    _FAISS_OK = True
except Exception:
    faiss = None
    _FAISS_OK = False

try:
    import networkx as nx
    _NX_OK = True
except Exception:
    nx = None
    _NX_OK = False

try:
    import hdbscan
    _HDBSCAN_OK = True
except Exception:
    hdbscan = None
    _HDBSCAN_OK = False

try:
    from sklearn.cluster import SpectralClustering, AgglomerativeClustering, DBSCAN
    from sklearn.mixture import GaussianMixture
    from sklearn.ensemble import IsolationForest
    _SKLEARN_ADVANCED_OK = True
except Exception:
    _SKLEARN_ADVANCED_OK = False

try:
    from sklearn.manifold import TSNE, UMAP
    _MANIFOLD_OK = True
except Exception:
    _MANIFOLD_OK = False

try:
    import torch
    import torchvision.transforms as transforms
    from torchvision.models import resnet50, ResNet50_Weights
    _TORCH_OK = True
except Exception:
    _TORCH_OK = False

from sklearn.neighbors import NearestNeighbors  # fallback for kNN
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import cdist, pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

# Optional insightface with graceful fallback
try:
    from insightface.app import FaceAnalysis
    _INSIGHTFACE_OK = True
except Exception as e:
    print(f"‚ö†Ô∏è InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    FaceAnalysis = None
    _INSIGHTFACE_OK = False

# -------------------------------
# Config / Defaults
# -------------------------------
IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

# Quality gates (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è)
MIN_DET_SCORE = 0.50  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Ü
MIN_BLUR_VAR = 50.0   # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ä–µ–∑–∫–æ—Å—Ç–∏

# Hi-Precision graph params (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
KNN_K = 60           # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Å–µ–¥—Å—Ç–≤–∞
T_STRICT = 0.65      # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Ä—ë–±–µ—Ä
T_MEMBER = 0.60      # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Ü –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
T_MERGE = 0.70       # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–ª–∏—è–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞)
DEGREE_TARGET = (3, 8)   # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –ø–ª–æ—Ç–Ω—ã—Ö —Å–≤—è–∑–µ–π
MUTUAL_RANK   = 8        # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ –≤–∑–∞–∏–º–Ω–æ–≥–æ —Ä–∞–Ω–≥–∞
MIN_SHARED_NEIGHBORS = 2 # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Å–µ–¥—Å—Ç–≤–∞

# –û–±—â–∏–µ –∏–º–µ–Ω–∞ –¥–ª—è exclude/include
EXCLUDED_NAMES = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]

# -------------------------------
# Utils
# -------------------------------

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS


def _win_long(path: Path) -> str:
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p


def _safe_path(p: Path) -> str:
    return _win_long(p)


def _safe_move(src: Path, dst: Path):
    shutil.move(_safe_path(src), _safe_path(dst))


def _safe_copy(src: Path, dst: Path):
    shutil.copy2(_safe_path(src), _safe_path(dst))


def imread_safe(path: Path):
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        return cv2.imdecode(data, cv2.IMREAD_COLOR)
    except Exception:
        return None


def _parse_cluster_id(name: str) -> Optional[int]:
    import re
    m = re.match(r"^\s*(\d+)", name)
    return int(m.group(1)) if m else None


def _collect_existing_numeric_ids(parent_dir: Path) -> Set[int]:
    ids = set()
    for d in parent_dir.iterdir():
        if d.is_dir():
            cid = _parse_cluster_id(d.name)
            if cid is not None:
                ids.add(cid)
    return ids


def _blur_var(img: np.ndarray) -> float:
    try:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    except Exception:
        return 0.0
    return float(cv2.Laplacian(gray, cv2.CV_64F).var())


def _combine_face_clothing_features(
    face_embeddings: np.ndarray, 
    clothing_features: List[np.ndarray],
    face_qualities: List[Dict] = None,
    clothing_weight: float = 0.3
) -> np.ndarray:
    """
    –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª–∏—Ü–∞ –∏ –æ–¥–µ–∂–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    """
    if not clothing_features or len(clothing_features) == 0:
        return face_embeddings
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ –º–∞—Å—Å–∏–≤
    clothing_array = np.array(clothing_features)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–¥–µ–∂–¥—ã
    clothing_norms = np.linalg.norm(clothing_array, axis=1, keepdims=True)
    clothing_array = clothing_array / (clothing_norms + 1e-8)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–∏—Ü–∞ –∏ –æ–¥–µ–∂–¥—ã
    # –í–µ—Å–∞: 70% –ª–∏—Ü–æ, 30% –æ–¥–µ–∂–¥–∞
    face_weight = 1.0 - clothing_weight
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã
    min_len = min(len(face_embeddings), len(clothing_array))
    face_embeddings = face_embeddings[:min_len]
    clothing_array = clothing_array[:min_len]
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    combined_features = np.concatenate([
        face_embeddings * face_weight,
        clothing_array * clothing_weight
    ], axis=1)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    combined_norms = np.linalg.norm(combined_features, axis=1, keepdims=True)
    combined_features = combined_features / (combined_norms + 1e-8)
    
    return combined_features


def _extract_advanced_features(X: np.ndarray, face_qualities: List[Dict] = None) -> np.ndarray:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    """
    N, D = X.shape
    features = []
    
    # 1. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    features.append(X)
    
    # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    # –°—Ä–µ–¥–Ω–µ–µ, std, min, max –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–º–µ—Ä–µ–Ω–∏—é
    stats_features = np.column_stack([
        np.mean(X, axis=1),
        np.std(X, axis=1),
        np.min(X, axis=1),
        np.max(X, axis=1),
        np.median(X, axis=1)
    ])
    features.append(stats_features)
    
    # 3. –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤
    centroid = np.mean(X, axis=0)
    centroid_dist = np.linalg.norm(X - centroid, axis=1, keepdims=True)
    features.append(centroid_dist)
    
    # 4. –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ (kNN distances)
    if N > 5:
        try:
            nbrs = NearestNeighbors(n_neighbors=min(5, N-1), metric='cosine')
            nbrs.fit(X)
            distances, _ = nbrs.kneighbors(X)
            local_density = np.mean(distances[:, 1:], axis=1, keepdims=True)  # –ò—Å–∫–ª—é—á–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å–µ–±—è
            features.append(local_density)
        except:
            pass
    
    # 5. –ö–∞—á–µ—Å—Ç–≤–æ –ª–∏—Ü –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if face_qualities and len(face_qualities) == N:
        quality_features = np.array([
            [q.get('total_score', 0.5), q.get('blur_score', 0.5), 
             q.get('pose_score', 0.5), q.get('brightness_score', 0.5)]
            for q in face_qualities
        ])
        features.append(quality_features)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    if features:
        combined_features = np.hstack(features)
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        combined_features = scaler.fit_transform(combined_features)
        return combined_features
    
    return X


def _ensemble_clustering_advanced(
    X: np.ndarray,
    face_qualities: List[Dict] = None,
    progress_callback=None,
) -> List[List[int]]:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ensemble –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
    """
    N = X.shape[0]
    if N < 2:
        return [[0]] if N == 1 else []
    
    if progress_callback:
        progress_callback("üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ensemble –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...", 75)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    X_enhanced = _extract_advanced_features(X, face_qualities)
    
    clustering_results = []
    weights = []
    
    # 1. HDBSCAN —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if _HDBSCAN_OK and hdbscan is not None:
        try:
            if progress_callback:
                progress_callback("üî¨ HDBSCAN ensemble...", 76)
            
            # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ HDBSCAN
            hdbscan_configs = [
                {'min_cluster_size': max(2, N//10), 'min_samples': 1, 'alpha': 1.0},
                {'min_cluster_size': max(2, N//8), 'min_samples': 1, 'alpha': 1.2},
                {'min_cluster_size': max(2, N//6), 'min_samples': 2, 'alpha': 0.8},
            ]
            
            for config in hdbscan_configs:
                try:
                    distances = 1.0 - np.dot(X, X.T)
                    distances = distances.astype(np.float64)
                    distances = (distances + distances.T) / 2.0
                    
                    clusterer = hdbscan.HDBSCAN(
                        metric='precomputed',
                        cluster_selection_method='eom',
                        allow_single_cluster=True,
                        **config
                    )
                    
                    labels = clusterer.fit_predict(distances)
                    clusters = _labels_to_clusters(labels)
                    if clusters:
                        clustering_results.append(clusters)
                        weights.append(0.3)  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è HDBSCAN
                except:
                    continue
        except:
            pass
    
    # 2. Spectral Clustering
    if _SKLEARN_ADVANCED_OK and N >= 3:
        try:
            if progress_callback:
                progress_callback("üåà –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è...", 78)
            
            # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            n_clusters_options = [max(2, N//8), max(2, N//6), max(2, N//4)]
            
            for n_clusters in n_clusters_options:
                if n_clusters >= N:
                    continue
                    
                try:
                    spectral = SpectralClustering(
                        n_clusters=n_clusters,
                        affinity='cosine',
                        assign_labels='kmeans',
                        random_state=42
                    )
                    labels = spectral.fit_predict(X)
                    clusters = _labels_to_clusters(labels)
                    if clusters:
                        clustering_results.append(clusters)
                        weights.append(0.2)
                except:
                    continue
        except:
            pass
    
    # 3. Agglomerative Clustering
    if _SKLEARN_ADVANCED_OK and N >= 3:
        try:
            if progress_callback:
                progress_callback("üå≥ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è...", 80)
            
            # –†–∞–∑–Ω—ã–µ linkage –º–µ—Ç–æ–¥—ã
            linkage_methods = ['ward', 'complete', 'average']
            
            for linkage_method in linkage_methods:
                try:
                    n_clusters = max(2, min(N//4, 8))  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ
                    agglo = AgglomerativeClustering(
                        n_clusters=n_clusters,
                        linkage=linkage_method,
                        metric='cosine'
                    )
                    labels = agglo.fit_predict(X)
                    clusters = _labels_to_clusters(labels)
                    if clusters:
                        clustering_results.append(clusters)
                        weights.append(0.15)
                except:
                    continue
        except:
            pass
    
    # 4. Gaussian Mixture Models
    if _SKLEARN_ADVANCED_OK and N >= 3:
        try:
            if progress_callback:
                progress_callback("üéØ Gaussian Mixture Models...", 82)
            
            n_components_options = [max(2, N//12), max(2, N//8), max(2, N//6)]
            
            for n_components in n_components_options:
                if n_components >= N:
                    continue
                    
                try:
                    gmm = GaussianMixture(
                        n_components=n_components,
                        covariance_type='full',
                        random_state=42,
                        max_iter=100
                    )
                    labels = gmm.fit_predict(X_enhanced)
                    clusters = _labels_to_clusters(labels)
                    if clusters:
                        clustering_results.append(clusters)
                        weights.append(0.1)
                except:
                    continue
        except:
            pass
    
    # 5. Consensus Clustering
    if len(clustering_results) >= 2:
        if progress_callback:
            progress_callback("ü§ù –£–º–Ω–∞—è consensus –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è...", 85)
        
        try:
            consensus_clusters = _consensus_clustering(clustering_results, weights, N, X)
            if consensus_clusters:
                return consensus_clusters
        except:
            pass
    
    # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if clustering_results:
        # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –≤–µ—Å–æ–º
        best_idx = np.argmax(weights)
        return clustering_results[best_idx]
    
    return []


def _labels_to_clusters(labels: np.ndarray) -> List[List[int]]:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç labels –≤ —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    clusters = defaultdict(list)
    for idx, label in enumerate(labels):
        if label != -1:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —à—É–º
            clusters[label].append(idx)
    
    return [sorted(cluster) for cluster in clusters.values() if len(cluster) >= 1]


def _consensus_clustering(
    clustering_results: List[List[List[int]]],
    weights: List[float],
    N: int,
    X: np.ndarray = None,
) -> List[List[int]]:
    """
    –£–º–Ω–∞—è consensus –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ —Å–ª–∏—è–Ω–∏–µ–º
    """
    if not clustering_results:
        return []
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarity_matrix = np.zeros((N, N))
    
    for clusters, weight in zip(clustering_results, weights):
        for cluster in clusters:
            for i in cluster:
                for j in cluster:
                    if i != j:
                        similarity_matrix[i, j] += weight
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    max_sim = similarity_matrix.max()
    if max_sim > 0:
        similarity_matrix = similarity_matrix / max_sim
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
    if X is not None:
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        cosine_sims = np.dot(X, X.T)
        np.fill_diagonal(cosine_sims, 0)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        threshold = np.percentile(similarity_matrix[similarity_matrix > 0], 30)  # 30-–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å
        threshold = max(threshold, 0.3)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
    else:
        threshold = 0.4  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
    
    consensus_edges = similarity_matrix >= threshold
    
    # –ù–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    if _NX_OK and nx is not None:
        G = nx.Graph()
        G.add_nodes_from(range(N))
        for i in range(N):
            for j in range(i+1, N):
                if consensus_edges[i, j]:
                    G.add_edge(i, j)
        
        components = list(nx.connected_components(G))
        clusters = [sorted(list(comp)) for comp in components if len(comp) >= 1]
    else:
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ NetworkX
        visited = [False] * N
        clusters = []
        
        for i in range(N):
            if not visited[i]:
                cluster = []
                stack = [i]
                
                while stack:
                    node = stack.pop()
                    if not visited[node]:
                        visited[node] = True
                        cluster.append(node)
                        
                        for j in range(N):
                            if not visited[j] and consensus_edges[node, j]:
                                stack.append(j)
                
                if len(cluster) >= 1:
                    clusters.append(sorted(cluster))
    
    # –£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if X is not None and len(clusters) > 1:
        clusters = _smart_merge_clusters(X, clusters)
    
    return clusters


def _smart_merge_clusters(X: np.ndarray, clusters: List[List[int]]) -> List[List[int]]:
    """
    –£–º–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
    """
    if len(clusters) <= 1:
        return clusters
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    centroids = []
    for cluster in clusters:
        cluster_embeddings = X[cluster]
        centroid = np.mean(cluster_embeddings, axis=0)
        centroid = centroid / (np.linalg.norm(centroid) + 1e-8)
        centroids.append(centroid)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å–ª–∏—è–Ω–∏—è
    merged_clusters = []
    used = [False] * len(clusters)
    
    for i in range(len(clusters)):
        if used[i]:
            continue
            
        current_cluster = clusters[i].copy()
        used[i] = True
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è
        for j in range(i + 1, len(clusters)):
            if used[j]:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤
            similarity = float(np.dot(centroids[i], centroids[j]))
            
            # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ª–∏—è–Ω–∏—è
            merge_threshold = 0.65  # –°–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–ª–∏—è–Ω–∏—è
            
            if similarity >= merge_threshold:
                current_cluster.extend(clusters[j])
                used[j] = True
        
        merged_clusters.append(sorted(current_cluster))
    
    return merged_clusters


def _extract_clothing_features(img: np.ndarray, face_bbox: tuple) -> np.ndarray:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–¥–µ–∂–¥—ã –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    if not _TORCH_OK:
        return np.zeros(512, dtype=np.float32)  # Fallback
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ResNet50 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if not hasattr(_extract_clothing_features, 'model'):
            _extract_clothing_features.model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
            _extract_clothing_features.model.eval()
            _extract_clothing_features.transform = transforms.Compose([
                transforms.ToPILImage(),
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–ª–∞—Å—Ç—å –æ–¥–µ–∂–¥—ã (–Ω–∏–∂–µ –ª–∏—Ü–∞)
        x1, y1, x2, y2 = face_bbox
        face_height = y2 - y1
        face_width = x2 - x1
        
        # –û–±–ª–∞—Å—Ç—å –æ–¥–µ–∂–¥—ã: —Ä–∞—Å—à–∏—Ä—è–µ–º bbox –ª–∏—Ü–∞ –≤–Ω–∏–∑
        clothing_x1 = max(0, int(x1 - face_width * 0.2))
        clothing_y1 = min(img.shape[0], int(y2 + face_height * 0.1))
        clothing_x2 = min(img.shape[1], int(x2 + face_width * 0.2))
        clothing_y2 = min(img.shape[0], int(y2 + face_height * 1.5))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±–ª–∞—Å—Ç—å –æ–¥–µ–∂–¥—ã —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if clothing_y1 >= clothing_y2 or clothing_x1 >= clothing_x2:
            return np.zeros(512, dtype=np.float32)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–ª–∞—Å—Ç—å –æ–¥–µ–∂–¥—ã
        clothing_crop = img[clothing_y1:clothing_y2, clothing_x1:clothing_x2]
        
        if clothing_crop.size == 0:
            return np.zeros(512, dtype=np.float32)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(clothing_crop.shape) == 3 and clothing_crop.shape[2] == 3:
            clothing_crop = cv2.cvtColor(clothing_crop, cv2.COLOR_BGR2RGB)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        clothing_tensor = _extract_clothing_features.transform(clothing_crop).unsqueeze(0)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        with torch.no_grad():
            features = _extract_clothing_features.model.avgpool(
                _extract_clothing_features.model.layer4(
                    _extract_clothing_features.model.layer3(
                        _extract_clothing_features.model.layer2(
                            _extract_clothing_features.model.layer1(
                                _extract_clothing_features.model.maxpool(
                                    _extract_clothing_features.model.relu(
                                        _extract_clothing_features.model.bn1(
                                            _extract_clothing_features.model.conv1(clothing_tensor)
                                        )
                                    )
                                )
                            )
                        )
                    )
                )
            )
            features = features.squeeze().numpy()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = features / (np.linalg.norm(features) + 1e-8)
        
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ 512 –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if len(features) > 512:
            features = features[:512]
        elif len(features) < 512:
            features = np.pad(features, (0, 512 - len(features)), 'constant')
        
        return features.astype(np.float32)
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–¥–µ–∂–¥—ã: {e}")
        return np.zeros(512, dtype=np.float32)


def _analyze_clothing_color(img: np.ndarray, face_bbox: tuple) -> Dict[str, float]:
    """
    –ê–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–æ–≤ –æ–¥–µ–∂–¥—ã
    """
    try:
        x1, y1, x2, y2 = face_bbox
        face_height = y2 - y1
        
        # –û–±–ª–∞—Å—Ç—å –æ–¥–µ–∂–¥—ã
        clothing_x1 = max(0, int(x1 - (x2-x1) * 0.2))
        clothing_y1 = min(img.shape[0], int(y2 + face_height * 0.1))
        clothing_x2 = min(img.shape[1], int(x2 + (x2-x1) * 0.2))
        clothing_y2 = min(img.shape[0], int(y2 + face_height * 1.2))
        
        if clothing_y1 >= clothing_y2 or clothing_x1 >= clothing_x2:
            return {'dominant_color': 0.0, 'color_variance': 0.0, 'brightness': 0.5}
        
        clothing_crop = img[clothing_y1:clothing_y2, clothing_x1:clothing_x2]
        
        if clothing_crop.size == 0:
            return {'dominant_color': 0.0, 'color_variance': 0.0, 'brightness': 0.5}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ HSV –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ü–≤–µ—Ç–∞
        hsv = cv2.cvtColor(clothing_crop, cv2.COLOR_BGR2HSV)
        
        # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Ü–≤–µ—Ç (Hue)
        hist_h = cv2.calcHist([hsv], [0], None, [180], [0, 180])
        dominant_hue = np.argmax(hist_h) / 180.0
        
        # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ü–≤–µ—Ç–∞
        color_variance = np.var(hsv[:, :, 0]) / 180.0
        
        # –Ø—Ä–∫–æ—Å—Ç—å
        brightness = np.mean(hsv[:, :, 2]) / 255.0
        
        return {
            'dominant_color': dominant_hue,
            'color_variance': color_variance,
            'brightness': brightness
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ü–≤–µ—Ç–∞ –æ–¥–µ–∂–¥—ã: {e}")
        return {'dominant_color': 0.0, 'color_variance': 0.0, 'brightness': 0.5}


def _assess_face_quality(face, img: np.ndarray) -> Dict[str, float]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    quality = {
        'det_score': getattr(face, 'det_score', 1.0),
        'blur_score': 0.0,
        'pose_score': 1.0,
        'occlusion_score': 1.0,
        'brightness_score': 1.0,
        'total_score': 0.0
    }
    
    try:
        # 1. –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏ (–±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è)
        x1, y1, x2, y2 = map(int, face.bbox)
        x1 = max(0, x1); y1 = max(0, y1)
        x2 = min(img.shape[1], x2); y2 = min(img.shape[0], y2)
        crop = img[y1:y2, x1:x2]
        
        if crop.size > 0:
            # Laplacian variance –¥–ª—è —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏
            blur_var = _blur_var(crop)
            quality['blur_score'] = min(1.0, blur_var / 200.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            # –Ø—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY) if len(crop.shape) == 3 else crop
            brightness = np.mean(gray) / 255.0
            # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å 0.3-0.7
            quality['brightness_score'] = 1.0 - abs(brightness - 0.5) * 2.0
        
        # 2. –û—Ü–µ–Ω–∫–∞ –ø–æ–∑—ã –≥–æ–ª–æ–≤—ã (—á–µ—Ä–µ–∑ landmarks –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        if hasattr(face, 'pose'):
            pose = face.pose
            # –ò–¥–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∞ = —Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –ª–∏—Ü–æ
            # pose –æ–±—ã—á–Ω–æ [pitch, yaw, roll]
            if pose is not None and len(pose) >= 2:
                yaw, pitch = abs(pose[0]), abs(pose[1])
                # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç —Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–∞
                quality['pose_score'] = max(0.0, 1.0 - (yaw + pitch) / 90.0)
        
        # 3. –û—Ü–µ–Ω–∫–∞ –æ–∫–∫–ª—é–∑–∏–∏ (—á–µ—Ä–µ–∑ landmark confidence –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if hasattr(face, 'landmark_3d_68'):
            # –ï—Å–ª–∏ –µ—Å—Ç—å 3D landmarks, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            quality['occlusion_score'] = 0.9  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        # 4. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞)
        weights = {
            'det_score': 0.3,
            'blur_score': 0.25,
            'pose_score': 0.25,
            'brightness_score': 0.1,
            'occlusion_score': 0.1
        }
        
        quality['total_score'] = sum(quality[k] * weights[k] for k in weights.keys())
        
    except Exception as e:
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –æ—Ü–µ–Ω–∫—É
        quality['total_score'] = quality['det_score'] * 0.5
    
    return quality


# -------------------------------
# Face embeddings extraction
# -------------------------------

def extract_embeddings(
    image_paths: List[Path],
    providers: List[str] = ("CPUExecutionProvider",),
    det_size=(640, 640),
    min_score: float = 0.3,  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏
    min_blur_var: float = MIN_BLUR_VAR,
    min_quality_score: float = 0.4,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    include_clothing: bool = True,  # –í–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ–¥–µ–∂–¥—ã
    progress_callback=None,
) -> Tuple[np.ndarray, List[Path], Dict[Path, int], List[Path], List[Path], List[Dict], List[np.ndarray]]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –æ–¥–µ–∂–¥—ã
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (X, owners, img_face_count, unreadable, no_faces, face_qualities, clothing_features)
      - X: np.ndarray [N, D] L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ª–∏—Ü
      - owners: list[Path] –¥–ª–∏–Ω—ã N (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ ‚Üí –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª)
      - img_face_count: dict[Path] ‚Üí –∫–æ–ª-–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ª–∏—Ü –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
      - unreadable: —Å–ø–∏—Å–æ–∫ –±–∏—Ç—ã—Ö/–Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
      - no_faces: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –ø—Ä–∏–≥–æ–¥–Ω—ã—Ö –ª–∏—Ü
      - face_qualities: —Å–ø–∏—Å–æ–∫ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Ü–∞
      - clothing_features: —Å–ø–∏—Å–æ–∫ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –æ–¥–µ–∂–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–∏—Ü–∞
    """
    if not _INSIGHTFACE_OK or FaceAnalysis is None:
        if progress_callback:
            progress_callback("‚ùå InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install insightface", 0)
        raise RuntimeError("InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install insightface")
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        app = FaceAnalysis(name="buffalo_l", providers=list(providers))
        ctx_id = -1 if "cpu" in str(providers).lower() else 0
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        app.prepare(ctx_id=ctx_id, det_size=(1024, 1024))
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InsightFace: {str(e)}", 0)
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InsightFace: {str(e)}")

    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...", 10)

    embeddings: List[np.ndarray] = []
    owners: List[Path] = []
    face_qualities: List[Dict] = []
    clothing_features: List[np.ndarray] = []
    img_face_count: Dict[Path, int] = {}
    unreadable: List[Path] = []
    no_faces: List[Path] = []

    total = len(image_paths)

    for i, p in enumerate(image_paths):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total, 1) * 60)  # 10-70
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {percent}% ({i+1}/{total}) - {p.name}", percent)

        img = imread_safe(p)
        if img is None:
            unreadable.append(p)
            continue

        faces = app.get(img)
        if not faces:
            no_faces.append(p)
            continue

        used = 0
        for f in faces:
            # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª–∏—Ü–∞
            quality = _assess_face_quality(f, img)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–¥–µ–∂–¥—ã
            if include_clothing:
                clothing_feat = _extract_clothing_features(img, f.bbox)
                clothing_color = _analyze_clothing_color(img, f.bbox)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–¥–µ–∂–¥—ã
                combined_clothing = np.concatenate([
                    clothing_feat,
                    np.array([clothing_color['dominant_color'], 
                             clothing_color['color_variance'], 
                             clothing_color['brightness']], dtype=np.float32)
                ])
                clothing_features.append(combined_clothing)
            else:
                clothing_features.append(np.zeros(515, dtype=np.float32))  # 512 + 3
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏—Ç–æ–≥–æ–≤–æ–π –æ—Ü–µ–Ω–∫–µ –∫–∞—á–µ—Å—Ç–≤–∞
            if quality['total_score'] < min_quality_score:
                continue

            emb = getattr(f, "normed_embedding", None)
            if emb is None:
                continue
            emb = emb.astype(np.float32)
            
            # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            n = np.linalg.norm(emb)
            if n <= 1e-6:  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–ª—è
                continue
            emb = emb / n
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            if np.any(np.isnan(emb)) or np.any(np.isinf(emb)):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–Ω–µ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
            if np.std(emb) < 1e-6:
                continue

            embeddings.append(emb)
            owners.append(p)
            face_qualities.append(quality)
            used += 1

        if used > 0:
            img_face_count[p] = used
        else:
            no_faces.append(p)

    if not embeddings:
        return np.empty((0, 512), dtype=np.float32), owners, img_face_count, unreadable, no_faces, []

    X = np.vstack(embeddings).astype(np.float32)
    
    # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è FAISS
    if _FAISS_OK:
        try:
            faiss.normalize_L2(X)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FAISS –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            # Fallback –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            norms = np.linalg.norm(X, axis=1, keepdims=True)
            X = X / (norms + 1e-8)
    else:
        # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä—É—á–Ω—É—é
        norms = np.linalg.norm(X, axis=1, keepdims=True)
        X = X / (norms + 1e-8)
    
    return X, owners, img_face_count, unreadable, no_faces, face_qualities, clothing_features


# -------------------------------
# kNN Graph + Mutual Edges + Components
# -------------------------------

def _knn_faiss(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
    d = X.shape[1]
    index = faiss.IndexHNSWFlat(d, 32)
    index.hnsw.efConstruction = 200
    index.add(X)
    index.hnsw.efSearch = 128
    D, I = index.search(X, k)
    # D ‚Äî squared L2 –ø—Ä–∏ IndexHNSWFlat; –Ω–æ –ø–æ—Å–ª–µ normalize_L2 dot –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫ 2-0.5*D? –ù–µ—Ç. –î–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ–º sim –∫–∞–∫ dot.
    # –ü–µ—Ä–µ—Å—á—ë—Ç sim –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º I.
    return D, I


def _knn_sklearn_cosine(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
    # cosine distance ‚Üí 1 - cosine similarity
    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='cosine')
    nbrs.fit(X)
    distances, indices = nbrs.kneighbors(X)
    # –ü—Ä–µ–≤—Ä–∞—Ç–∏–º –≤ similarity
    sims = 1.0 - distances
    return sims, indices


def build_mutual_edges(
    X: np.ndarray,
    k: int = KNN_K,
    t_strict: Optional[float] = None,  # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
    mutual_rank: int = MUTUAL_RANK,
    min_shared_neighbors: int = MIN_SHARED_NEIGHBORS,
) -> List[Tuple[int, int, float]]:
    """–°—Ç—Ä–æ–∏–º —Å–ø–∏—Å–æ–∫ –Ω–∞–¥—ë–∂–Ω—ã—Ö —Ä—ë–±–µ—Ä —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º.
    1) kNN (FAISS/Sklearn)
    2) –ª–æ–∫–∞–ª—å–Ω–∞—è robust z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–æ–±–∏—è
    3) —É—Å–∏–ª–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é (Jaccard –æ–±—â–∏—Ö —Å–æ—Å–µ–¥–µ–π)
    4) –∞–≤—Ç–æ–ø–æ—Ä–æ–≥ –ø–æ —Ü–µ–ª–µ–≤–æ–π —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–µ–ø–µ–Ω–∏ DEGREE_TARGET
    """
    N = X.shape[0]
    k = min(k, max(2, N))

    # 1) kNN
    if _FAISS_OK and N >= 1000:
        _, I = _knn_faiss(X, k)
        sims = np.zeros((N, k), dtype=np.float32)
        for i in range(N):
            sims[i] = np.dot(X[I[i]].astype(np.float32), X[i].astype(np.float32))
    else:
        sims, I = _knn_sklearn_cosine(X, k)

    # 2) –ª–æ–∫–∞–ª—å–Ω–∞—è robust z-–Ω–æ—Ä–º–∞ (–±–µ–∑ self-—Å—Ç–æ–ª–±—Ü–∞)
    neigh = sims[:, 1:]
    med = np.median(neigh, axis=1, keepdims=True)
    mad = np.median(np.abs(neigh - med), axis=1, keepdims=True)
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    # med_full –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ sims
    med_full = np.concatenate([np.zeros((N,1),dtype=np.float32), np.tile(med, (1, k-1))], axis=1)
    mad_full = np.concatenate([np.ones((N,1),dtype=np.float32), np.tile(mad, (1, k-1))], axis=1)
    z = (sims - med_full) / (1.4826*(mad_full+1e-6))

    neighbor_sets = [set(I[i, 1:]) for i in range(N)]
    rank = [{int(I[i, r]): r for r in range(1, I.shape[1])} for i in range(N)]

    # 3) –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º z –∏ Jaccard
    cand = []  # (i, j, w)
    for i in range(N):
        for jpos, j in enumerate(I[i, 1:]):
            if j < 0 or j == i:
                continue
            # –≤–∑–∞–∏–º–Ω—ã–π TOP-R
            ri = rank[i].get(j, 10**9)
            rj = rank[j].get(i, 10**9)
            if ri > mutual_rank or rj > mutual_rank:
                continue
            # –æ–±—â–∏–µ —Å–æ—Å–µ–¥–∏
            inter = len(neighbor_sets[i].intersection(neighbor_sets[j]))
            if inter < min_shared_neighbors:
                continue
            # —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π z
            zj_i = float(z[i, jpos+1])
            j_rank = rank[j].get(i, None)
            zj_j = float(z[j, j_rank]) if j_rank is not None else 0.0
            zsym = 0.5*(zj_i + zj_j)
            # Jaccard
            jac = inter / max(1, len(neighbor_sets[i].union(neighbor_sets[j])))
            w = zsym * (0.5 + 0.5*jac)
            cand.append((i, j, w))

    if not cand:
        return []

    # –°–∏–º–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—è –ø–∞—Ä (i<j)
    pair = {}
    for i, j, w in cand:
        a, b = (i, j) if i < j else (j, i)
        pair.setdefault((a, b), []).append(w)
    scores = np.array([np.mean(ws) for ws in pair.values()], dtype=np.float32)

    # 4) –∞–≤—Ç–æ–ø–æ—Ä–æ–≥ –ø–æ DEGREE_TARGET
    def avg_degree(th):
        m = 0
        for w in scores:
            if w >= th:
                m += 1
        return (2.0*m) / max(1, N)

    low, high = np.percentile(scores, 10), np.percentile(scores, 90)
    target_lo, target_hi = DEGREE_TARGET
    t = (low + high) / 2.0
    for _ in range(20):
        deg = avg_degree(t)
        if deg < target_lo:
            high = t
        elif deg > target_hi:
            low = t
        else:
            break
        t = (low + high) / 2.0

    edges = []
    idx = 0
    for (a, b), wlist in pair.items():
        score = float(scores[idx]); idx += 1
        if score >= t:
            edges.append((a, b, score))
    return edges


def connected_components_from_edges(N: int, edges: List[Tuple[int, int, float]]) -> List[List[int]]:
    if _NX_OK:
        G = nx.Graph()
        G.add_nodes_from(range(N))
        for u, v, w in edges:
            G.add_edge(u, v, weight=w)
        return [list(c) for c in nx.connected_components(G)]
    # Fallback: union-find (–±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
    parent = list(range(N))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for u, v, _ in edges:
        union(u, v)

    groups = defaultdict(list)
    for i in range(N):
        groups[find(i)].append(i)
    return list(groups.values())


# -------------------------------
# Medoid membership filtering & optional inter-cluster merge
# -------------------------------

def _medoid_index(points: np.ndarray) -> int:
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—É–º–º–æ–π –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Å—Ö–æ–¥—Å—Ç–≤ –∫ –¥—Ä—É–≥–∏–º
    # —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ argmax –ø–æ —Å—É–º–º–µ dot, —Ç–∞–∫ –∫–∞–∫ X —É–∂–µ L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω
    S = np.dot(points, points.T)
    sums = S.sum(axis=1)
    return int(np.argmax(sums))


def filter_by_medoid(
    X: np.ndarray,
    components: List[List[int]],
    t_member: float = T_MEMBER,
) -> List[List[int]]:
    filtered = []
    for comp in components:
        if len(comp) == 1:
            filtered.append(comp)
            continue
        P = X[comp]
        m_idx_local = _medoid_index(P)
        m = P[m_idx_local]
        sims = np.dot(P, m)
        keep = [comp[i] for i, s in enumerate(sims) if float(s) >= t_member]
        if len(keep) >= 1:
            filtered.append(keep)
    return filtered


def optional_merge_by_centroids(
    X: np.ndarray,
    clusters: List[List[int]],
    t_merge: float = T_MERGE,
    cluster_attrs: Optional[List[dict]] = None,
) -> List[List[int]]:
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –º–∞–∫—Å–∏–º—É–º–∞ precision —ç—Ç–æ—Ç —ç—Ç–∞–ø –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —É—Ç–∏–ª–∏—Ç—É)
    C = [np.mean(X[c], axis=0) for c in clusters]
    C = [c / (np.linalg.norm(c) + 1e-12) for c in C]

    merged = []
    used = [False] * len(clusters)

    def attrs_conflict(a, b) -> bool:
        if not cluster_attrs:
            return False
        A, B = cluster_attrs[a], cluster_attrs[b]
        # –ü—Ä–æ—Å—Ç–µ–π—à–∏–µ –≥–µ–π—Ç—ã: –µ—Å–ª–∏ –æ–±–∞ –∑–Ω–∞—é—Ç gender –∏ –æ–Ω–∏ —Ä–∞–∑–Ω—ã–µ ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç
        ga, gb = A.get('gender'), B.get('gender')
        if ga is not None and gb is not None and ga != gb:
            return True
        # –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –±–∏–Ω—ã: –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è —Å–∏–ª—å–Ω–æ ‚Äî –±–ª–æ–∫
        ya, yb = A.get('age'), B.get('age')
        if ya is not None and yb is not None and abs(ya - yb) >= 15:
            return True
        return False

    for i in range(len(clusters)):
        if used[i]:
            continue
        cur = list(clusters[i])
        used[i] = True
        for j in range(i + 1, len(clusters)):
            if used[j]:
                continue
            sim = float(np.dot(C[i], C[j]))
            if sim >= t_merge and not attrs_conflict(i, j):
                cur.extend(clusters[j])
                used[j] = True
        merged.append(sorted(cur))
    return merged


def hdbscan_cluster_professional(
    X: np.ndarray,
    face_qualities: List[Dict] = None,
    min_cluster_size: int = 2,
    min_samples: int = 1,
    progress_callback=None,
) -> List[List[int]]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å HDBSCAN
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç state-of-the-art –ø–æ–¥—Ö–æ–¥ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ª–∏—Ü
    """
    N = X.shape[0]
    if N == 0:
        return []
    
    if progress_callback:
        progress_callback("üß¨ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è)...", 75)
    
    if not _HDBSCAN_OK or hdbscan is None:
        # Fallback –Ω–∞ –≥—Ä–∞—Ñ–æ–≤—ã–π –º–µ—Ç–æ–¥
        if progress_callback:
            progress_callback("‚ö†Ô∏è HDBSCAN –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä–∞—Ñ–æ–≤—ã–π –º–µ—Ç–æ–¥", 76)
        return []
    
    try:
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (1 - cosine similarity)
        # HDBSCAN —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–æ–π —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        distances = 1.0 - np.dot(X, X.T)
        np.fill_diagonal(distances, 0)  # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å–µ–±—è = 0
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ç–∏–ø—É –¥–ª—è HDBSCAN (float64)
        distances = distances.astype(np.float64)
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–∞—Ç—Ä–∏—Ü–∞ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞
        distances = (distances + distances.T) / 2.0
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        adaptive_min_cluster_size = max(2, min(min_cluster_size, N // 20))
        adaptive_min_samples = max(1, min(min_samples, adaptive_min_cluster_size // 2))
        
        # HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=adaptive_min_cluster_size,
            min_samples=adaptive_min_samples,
            metric='precomputed',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º pre-computed distances
            cluster_selection_method='eom',  # Excess of Mass - –ª—É—á—à–∏–π –º–µ—Ç–æ–¥
            alpha=1.0,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
            allow_single_cluster=True,
            cluster_selection_epsilon=0.0,
        )
        
        if progress_callback:
            progress_callback("üî¨ –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 80)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
        labels = clusterer.fit_predict(distances)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏
        probabilities = clusterer.probabilities_ if hasattr(clusterer, 'probabilities_') else None
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
        clusters = defaultdict(list)
        for idx, label in enumerate(labels):
            if label != -1:  # -1 = —à—É–º
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
                if probabilities is not None:
                    if probabilities[idx] >= 0.5:  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        clusters[label].append(idx)
                else:
                    clusters[label].append(idx)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —à—É–º–∞ - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–∑–Ω–∞—á–∏—Ç—å –≤ –±–ª–∏–∂–∞–π—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        noise_indices = [i for i, label in enumerate(labels) if label == -1]
        if noise_indices and clusters:
            if progress_callback:
                progress_callback("üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤...", 85)
            
            for noise_idx in noise_indices:
                noise_emb = X[noise_idx]
                best_cluster = -1
                best_sim = 0.4  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                
                for cluster_id, cluster_indices in clusters.items():
                    if not cluster_indices:
                        continue
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –º–µ–¥–æ–∏–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–∞
                    cluster_embs = X[cluster_indices]
                    centroid = np.mean(cluster_embs, axis=0)
                    centroid = centroid / (np.linalg.norm(centroid) + 1e-8)
                    sim = float(np.dot(noise_emb, centroid))
                    
                    if sim > best_sim:
                        best_sim = sim
                        best_cluster = cluster_id
                
                if best_cluster != -1:
                    clusters[best_cluster].append(noise_idx)
        
        result = [sorted(indices) for indices in clusters.values() if len(indices) >= 1]
        
        if progress_callback:
            progress_callback(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(result)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 90)
        
        return result
        
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ HDBSCAN: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback", 76)
        return []


def post_process_clusters(
    X: np.ndarray,
    clusters: List[List[int]],
    face_qualities: List[Dict] = None,
    min_cluster_size: int = 2,
    progress_callback=None,
) -> List[List[int]]:
    """–£–º–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    if not clusters:
        return clusters
    
    if progress_callback:
        progress_callback("üîß –£–º–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 92)
    
    processed_clusters = []
    
    for cluster in clusters:
        if len(cluster) < min_cluster_size:
            processed_clusters.append(cluster)
            continue
            
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ–¥–∏–æ–∏–¥ (–Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç)
        cluster_embeddings = X[cluster]
        similarities = np.dot(cluster_embeddings, cluster_embeddings.T)
        sum_sims = similarities.sum(axis=1)
        medoid_idx_local = np.argmax(sum_sims)
        medoid = cluster_embeddings[medoid_idx_local]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –º–µ–¥–∏–æ–∏–¥–æ–º
        sims_to_medoid = np.dot(cluster_embeddings, medoid)
        
        # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        threshold = np.percentile(sims_to_medoid, 25)  # –û—Å—Ç–∞–≤–ª—è–µ–º 75% (–±—ã–ª–æ 85%)
        threshold = max(threshold, 0.4)  # –°–Ω–∏–∂–µ–Ω –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ (–±—ã–ª–æ 0.5)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
        filtered_indices = [cluster[i] for i, sim in enumerate(sims_to_medoid) if sim >= threshold]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if face_qualities and len(face_qualities) == len(X):
            quality_filtered = []
            avg_quality = np.mean([face_qualities[i]['total_score'] for i in filtered_indices])
            for idx in filtered_indices:
                if face_qualities[idx]['total_score'] >= avg_quality * 0.6:  # –°–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ (–±—ã–ª–æ 0.7)
                    quality_filtered.append(idx)
            if len(quality_filtered) >= min_cluster_size:
                filtered_indices = quality_filtered
        
        if len(filtered_indices) >= 1:
            processed_clusters.append(filtered_indices)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if len(processed_clusters) > 1:
        processed_clusters = _smart_merge_clusters(X, processed_clusters)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ over-clustering
    if len(processed_clusters) > max(1, len(X) // 3):  # –ú–∞–∫—Å–∏–º—É–º N/3 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        processed_clusters = _aggressive_merge_clusters(X, processed_clusters)
    
    return processed_clusters


def _aggressive_merge_clusters(X: np.ndarray, clusters: List[List[int]]) -> List[List[int]]:
    """
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è over-clustering
    """
    if len(clusters) <= 1:
        return clusters
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã
    centroids = []
    for cluster in clusters:
        cluster_embeddings = X[cluster]
        centroid = np.mean(cluster_embeddings, axis=0)
        centroid = centroid / (np.linalg.norm(centroid) + 1e-8)
        centroids.append(centroid)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è —Å –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
    merged_clusters = []
    used = [False] * len(clusters)
    
    for i in range(len(clusters)):
        if used[i]:
            continue
            
        current_cluster = clusters[i].copy()
        used[i] = True
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è —Å–ª–∏—è–Ω–∏—è
        for j in range(i + 1, len(clusters)):
            if used[j]:
                continue
                
            similarity = float(np.dot(centroids[i], centroids[j]))
            
            # –û—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ª–∏—è–Ω–∏—è
            if similarity >= 0.55:  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
                current_cluster.extend(clusters[j])
                used[j] = True
        
        merged_clusters.append(sorted(current_cluster))
    
    return merged_clusters


# -------------------------------
# High-Precision clustering (end-to-end)
# -------------------------------

def hi_precision_cluster(
    X: np.ndarray,
    face_qualities: List[Dict] = None,
    clothing_features: List[np.ndarray] = None,
    min_cluster_size: int = 2,
    min_samples: int = 1,
    t_member: float = T_MEMBER,
    allow_merge: bool = True,
    t_merge: float = T_MERGE,
    use_advanced_ensemble: bool = True,  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–º—É ensemble
    use_clothing: bool = True,
    clothing_weight: float = 0.3,
    progress_callback=None,
) -> List[List[int]]:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ª–∏—Ü —Å –∞–Ω–∞–ª–∏–∑–æ–º –æ–¥–µ–∂–¥—ã (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ensemble –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö state-of-the-art –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    """
    N = X.shape[0]
    if N == 0:
        return []
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–∏—Ü–∞ –∏ –æ–¥–µ–∂–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if use_clothing and clothing_features:
        if progress_callback:
            progress_callback("üëï –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ª–∏—Ü–∞ –∏ –æ–¥–µ–∂–¥—ã...", 70)
        X = _combine_face_clothing_features(X, clothing_features, face_qualities, clothing_weight)

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (–æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä)
    if progress_callback:
        progress_callback("üîó –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥...", 75)
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–µ—à–µ–Ω–∏–µ: –≤—Å–µ –ª–∏—Ü–∞ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    if N <= 10:  # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ - –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
        if progress_callback:
            progress_callback("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏...", 90)
        return [list(range(N))]
    
    # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –≥—Ä–∞—Ñ–æ–≤—ã–π –º–µ—Ç–æ–¥
    edges = build_mutual_edges(X, k=min(5, N-1))  # –ú–∏–Ω–∏–º—É–º —Å–æ—Å–µ–¥–µ–π
    
    if progress_callback:
        progress_callback("üß© –°–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...", 82)
    comps = connected_components_from_edges(N, edges)
    
    if progress_callback:
        progress_callback("üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ–¥–∏–æ–∏–¥—É...", 88)
    clusters = filter_by_medoid(X, comps, t_member=0.3)  # –û—á–µ–Ω—å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
    
    if allow_merge:
        if progress_callback:
            progress_callback("üß¨ –°–ª–∏—è–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 90)
        clusters = optional_merge_by_centroids(X, clusters, t_merge=0.4, cluster_attrs=None)  # –û—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞
    clusters = post_process_clusters(
        X, clusters,
        face_qualities=face_qualities,
        min_cluster_size=1,
        progress_callback=progress_callback
    )
    
    return clusters
    
    # Fallback: –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ensemble –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if use_advanced_ensemble:
        clusters = _ensemble_clustering_advanced(
            X, 
            face_qualities=face_qualities,
            progress_callback=progress_callback
        )
        
        if clusters:  # –ï—Å–ª–∏ ensemble —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞
            clusters = post_process_clusters(
                X, clusters, 
                face_qualities=face_qualities,
                min_cluster_size=1, 
                progress_callback=progress_callback
            )
            return clusters
    
    # Fallback 1: HDBSCAN
    if _HDBSCAN_OK:
        if progress_callback:
            progress_callback("üî¨ HDBSCAN fallback...", 75)
        
        clusters = hdbscan_cluster_professional(
            X, 
            face_qualities=face_qualities,
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            progress_callback=progress_callback
        )
        
        if clusters:
            clusters = post_process_clusters(
                X, clusters, 
                face_qualities=face_qualities,
                min_cluster_size=1, 
                progress_callback=progress_callback
            )
            return clusters
    
    # Fallback 2: –≥—Ä–∞—Ñ–æ–≤—ã–π –º–µ—Ç–æ–¥
    if progress_callback:
        progress_callback("üîó –ì—Ä–∞—Ñ–æ–≤–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (fallback)...", 75)
    
    edges = build_mutual_edges(X, k=KNN_K)
    
    if progress_callback:
        progress_callback("üß© –°–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...", 82)
    comps = connected_components_from_edges(N, edges)
    
    if progress_callback:
        progress_callback("üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ–¥–∏–æ–∏–¥—É...", 88)
    clusters = filter_by_medoid(X, comps, t_member=t_member)
    
    if allow_merge:
        if progress_callback:
            progress_callback("üß¨ –°–ª–∏—è–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 90)
        clusters = optional_merge_by_centroids(X, clusters, t_merge=t_merge, cluster_attrs=None)
    
    # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞
    clusters = post_process_clusters(
        X, clusters,
        face_qualities=face_qualities,
        min_cluster_size=1,
        progress_callback=progress_callback
    )
    
    return clusters

# -------------------------------
# Public API: build_plan_live (Hi-Precision)
# -------------------------------

def build_plan_live(
    input_dir: Path,
    det_size=(1024, 1024),  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    min_score: float = MIN_DET_SCORE,
    min_cluster_size: int = 1,  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –ª–∏—Ü
    min_samples: int = 1,       # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –≥–∏–±–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    providers: List[str] = ("CPUExecutionProvider",),
    progress_callback=None,
    include_excluded: bool = False,
    allow_merge: bool = True,   # –í–∫–ª—é—á–∞–µ–º —Å–ª–∏—è–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    t_strict: float = T_STRICT,
    t_member: float = T_MEMBER,
    t_merge: float = T_MERGE,
):
    """
    Hi-Precision –≤–µ—Ä—Å–∏—è build_plan_live: –±–µ–∑ O(N^2), —Å –≥—Ä–∞—Ñ–æ–≤–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –º–µ–¥–∏–æ–∏–¥–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏: clusters, plan, unreadable, no_faces.
    """
    input_dir = Path(input_dir)
    if include_excluded:
        all_images = [p for p in input_dir.rglob('*') if is_image(p)]
    else:
        all_images = [p for p in input_dir.rglob('*') if is_image(p) and not any(ex in str(p).lower() for ex in EXCLUDED_NAMES)]

    if progress_callback:
        progress_callback(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è: {input_dir}, –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 1)

    # 1) –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    try:
        X, owners, img_face_count, unreadable, no_faces, face_qualities, clothing_features = extract_embeddings(
            all_images,
            providers=providers,
            det_size=det_size,
            min_score=0.3,  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏
            min_quality_score=0.35,  # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            include_clothing=True,
            progress_callback=progress_callback,
        )
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}", 100)
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in all_images],
            "no_faces": [],
            "error": str(e)
        }

    if X.shape[0] == 0:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", 100)
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    # 2) –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (Ensemble + HDBSCAN + fallback)
    if progress_callback:
        progress_callback(f"üîÑ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {X.shape[0]} –ª–∏—Ü...", 75)
    clusters_idx = hi_precision_cluster(
        X,
        face_qualities=face_qualities,
        clothing_features=clothing_features,
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        t_member=t_member,
        allow_merge=True,
        t_merge=t_merge,
        use_advanced_ensemble=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ensemble –∫–∞–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        use_clothing=True,
        clothing_weight=0.3,
        progress_callback=progress_callback,
    )

    # 3) –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—ã
    cluster_map: Dict[int, Set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)
    for new_label, comp in enumerate(clusters_idx):
        for idx in comp:
            p = owners[idx]
            cluster_map[new_label].add(p)
            cluster_by_img[p].add(new_label)

    # 4) –ü–ª–∞–Ω —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    if progress_callback:
        progress_callback("üì¶ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...", 95)
    plan = []
    # –û–±—Ö–æ–¥–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≤—ã–±–æ—Ä–∫–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∏ —É—á–µ—Å—Ç—å multi-face
    seen = set()
    for p in owners:
        if p in seen:
            continue
        seen.add(p)
        clusters = sorted(list(cluster_by_img.get(p, set())))
        if clusters:
            plan.append({
                "path": str(p),
                "cluster": clusters,
                "faces": img_face_count.get(p, 0)
            })

    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(plan)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 100)

    return {
        "clusters": {int(k): [str(x) for x in sorted(v, key=lambda s: str(s))] for k, v in cluster_map.items()},
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }


# -------------------------------
# Distribution (—Ñ–∏–∫—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ª–æ–≥–∏–∫–∏)
# -------------------------------

def distribute_to_folders(
    plan: dict,
    base_dir: Path,
    cluster_start: int = 1,
    progress_callback=None,
    keep_original_on_multi: bool = True,
    annotate_folders: bool = False
) -> Tuple[int, int, int]:
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)

    cluster_file_counts: Dict[int, int] = {}
    for item in plan_items:
        for cluster_id in (cluster_id_map[c] for c in item["cluster"]):
            cluster_file_counts[cluster_id] = cluster_file_counts.get(cluster_id, 0) + 1

    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)

        src = Path(item["path"])
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue

        if len(clusters) == 1:
            cluster_id = clusters[0]
            dst = base_dir / f"{cluster_id}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                try:
                    if src.resolve() == dst.resolve():
                        continue
                except Exception:
                    if str(src) == str(dst):
                        continue
                _safe_move(src, dst)
                moved += 1
                moved_paths.add(src.parent)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {src} ‚Üí {dst}: {e}")
        else:
            # multi: –∫–æ–ø–∏—Ä—É–µ–º –≤ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä, –æ—Ä–∏–≥–∏–Ω–∞–ª –æ—Å—Ç–∞–≤–ª—è–µ–º
            for cluster_id in clusters:
                dst = base_dir / f"{cluster_id}" / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    try:
                        if src.resolve() == dst.resolve():
                            continue
                    except Exception:
                        if str(src) == str(dst):
                            continue
                    _safe_copy(src, dst)
                    copied += 1
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src} ‚Üí {dst}: {e}")

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–µ–ª–∞–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî Off)
    if annotate_folders and cluster_file_counts:
        if progress_callback:
            progress_callback("üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–∞–ø–æ–∫...", 95)
        for cluster_id, file_count in cluster_file_counts.items():
            old_folder = base_dir / str(cluster_id)
            new_folder = base_dir / f"{cluster_id} ({file_count})"
            if old_folder.exists() and old_folder.is_dir():
                try:
                    old_folder.rename(new_folder)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è {old_folder} ‚Üí {new_folder}: {e}")

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)
    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            if p.exists() and not any(p.iterdir()):
                p.rmdir()
        except Exception:
            pass

    return moved, copied, cluster_start + len(used_clusters)


# -------------------------------
# "–û–±—â–∏–µ" –ø–∞–ø–∫–∏: –ø–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
# -------------------------------

def find_common_folders_recursive(root_dir: Path, max_depth: int = 3):
    common_folders = []

    def scan_directory(dir_path: Path, level=0):
        if level > max_depth:
            return
        try:
            for item in dir_path.iterdir():
                if item.is_dir():
                    low = item.name.lower()
                    if any(ex in low for ex in EXCLUDED_NAMES):
                        common_folders.append(item)
                    else:
                        scan_directory(item, level + 1)
        except PermissionError:
            print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–ø–∫–µ: {dir_path}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {dir_path}: {e}")

    scan_directory(Path(root_dir))
    return common_folders


def process_common_folder_at_level(common_dir: Path, progress_callback=None) -> int:
    parent_dir = common_dir.parent
    existing_ids = _collect_existing_numeric_ids(parent_dir)

    data = build_plan_live(common_dir, include_excluded=True, progress_callback=progress_callback)
    plan = data.get('plan', [])
    if not plan:
        return 0

    # –ú–∞–ø–ø–∏–Ω–≥: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–º –¥–∞—ë–º –Ω–æ–≤—ã–µ ID –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    used_clusters_src = sorted({c for item in plan for c in item['cluster']})
    next_id = (max(existing_ids) + 1) if existing_ids else 1
    cluster_id_map = {}
    for old in used_clusters_src:
        if old in existing_ids:
            cluster_id_map[old] = old
        else:
            cluster_id_map[old] = next_id
            next_id += 1

    remapped_plan = []
    for item in plan:
        remapped_plan.append({
            'path': item['path'],
            'cluster': sorted(cluster_id_map[c] for c in item['cluster']),
            'faces': item.get('faces', 0)
        })

    moved, copied, _ = distribute_to_folders(
        {"plan": remapped_plan},
        base_dir=parent_dir,
        cluster_start=1,
        progress_callback=progress_callback,
        keep_original_on_multi=True,
        annotate_folders=False,
    )
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–ª-–≤–æ –∫–æ–ø–∏–π –∫–∞–∫ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ä–∞–±–æ—Ç—ã
    return copied


def process_group_folder(group_dir: Path, progress_callback=None, include_excluded: bool = False):
    cluster_counter = 1
    group_dir = Path(group_dir)

    if include_excluded:
        if progress_callback:
            progress_callback("üîç –ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ' –≤–æ –≤—Å–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏...", 10)
        common_folders = find_common_folders_recursive(group_dir)
        if not common_folders:
            if progress_callback:
                progress_callback("‚ùå –ü–∞–ø–∫–∏ '–æ–±—â–∏–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã", 100)
            return 0, 0, cluster_counter

        total_copied = 0
        total = len(common_folders)
        for i, common_folder in enumerate(common_folders):
            if progress_callback:
                percent = 20 + int((i + 1) / total * 70)
                progress_callback(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è: {common_folder.name} ({i+1}/{total})", percent)
            total_copied += process_common_folder_at_level(common_folder, progress_callback)

        if progress_callback:
            progress_callback(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ '–æ–±—â–∏–µ': {total_copied} —Ñ–∞–π–ª–æ–≤", 100)
        return 0, total_copied, cluster_counter

    # –ò–Ω–∞—á–µ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ, –∏—Å–∫–ª—é—á–∞—è '–æ–±—â–∏–µ'
    subfolders = [f for f in sorted(group_dir.iterdir()) if f.is_dir() and "–æ–±—â–∏–µ" not in f.name.lower()]
    total_subfolders = len(subfolders)

    for i, subfolder in enumerate(subfolders):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total_subfolders, 1) * 80)
            progress_callback(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ({i+1}/{total_subfolders})", percent)

        plan = build_plan_live(subfolder, progress_callback=progress_callback)
        moved, copied, cluster_counter = distribute_to_folders(
            plan, subfolder, cluster_start=cluster_counter, progress_callback=progress_callback
        )

    return 0, 0, cluster_counter
