import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict

# Optional libs with graceful fallbacks
try:
    import faiss  # faiss-cpu
    _FAISS_OK = True
except Exception:
    faiss = None
    _FAISS_OK = False

try:
    import networkx as nx
    _NX_OK = True
except Exception:
    nx = None
    _NX_OK = False

from sklearn.neighbors import NearestNeighbors  # fallback for kNN

# Optional insightface with graceful fallback
try:
    from insightface.app import FaceAnalysis
    _INSIGHTFACE_OK = True
except Exception as e:
    print(f"‚ö†Ô∏è InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
    FaceAnalysis = None
    _INSIGHTFACE_OK = False

# -------------------------------
# Config / Defaults
# -------------------------------
IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

# Quality gates (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –º–∞–∫—Å–∏–º—É–º precision)
MIN_DET_SCORE = 0.80
MIN_BLUR_VAR = 120.0  # var(Laplacian) ‚Äì –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∑–∫–æ—Å—Ç–∏

# Hi-Precision graph params (–∫–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å–∏–º–º–µ—Ç—Ä–∏—è –Ω–∞ L2-–≤–µ–∫—Ç–æ—Ä–∞—Ö)
KNN_K = 40
T_STRICT = 0.80   # —Ä–µ–±—Ä–æ —Å–æ–∑–¥–∞—ë—Ç—Å—è –µ—Å–ª–∏ sim >= T_STRICT –∏ —Å–æ—Å–µ–¥—Å—Ç–≤–æ –≤–∑–∞–∏–º–Ω–æ–µ
T_MEMBER = 0.78   # —Ñ–∏–ª—å—Ç—Ä —É—á–∞—Å—Ç–Ω–∏–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–∏–æ–∏–¥–∞
T_MERGE = 0.82    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ª–∏—è–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ (–±–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞)
DEGREE_TARGET = (2, 4)   # –∂–µ–ª–∞–µ–º–∞—è —Å—Ä–µ–¥–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å –≥—Ä–∞—Ñ–∞ –ø–æ—Å–ª–µ –ø–æ—Ä–æ–≥–∞
MUTUAL_RANK   = 5        # –≤–∑–∞–∏–º–Ω—ã–π —Ä–∞–Ω–≥ —Å–æ—Å–µ–¥–∞ (–æ–±–∞ –≤ TOP-5 –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞)
MIN_SHARED_NEIGHBORS = 4 # –º–∏–Ω–∏–º—É–º –æ–±—â–∏—Ö —Å–æ—Å–µ–¥–µ–π (–ø–æ kNN-—Å–ø–∏—Å–∫–∞–º)

# –û–±—â–∏–µ –∏–º–µ–Ω–∞ –¥–ª—è exclude/include
EXCLUDED_NAMES = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]

# -------------------------------
# Utils
# -------------------------------

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS


def _win_long(path: Path) -> str:
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p


def _safe_path(p: Path) -> str:
    return _win_long(p)


def _safe_move(src: Path, dst: Path):
    shutil.move(_safe_path(src), _safe_path(dst))


def _safe_copy(src: Path, dst: Path):
    shutil.copy2(_safe_path(src), _safe_path(dst))


def imread_safe(path: Path):
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        return cv2.imdecode(data, cv2.IMREAD_COLOR)
    except Exception:
        return None


def _parse_cluster_id(name: str) -> Optional[int]:
    import re
    m = re.match(r"^\s*(\d+)", name)
    return int(m.group(1)) if m else None


def _collect_existing_numeric_ids(parent_dir: Path) -> Set[int]:
    ids = set()
    for d in parent_dir.iterdir():
        if d.is_dir():
            cid = _parse_cluster_id(d.name)
            if cid is not None:
                ids.add(cid)
    return ids


def _blur_var(img: np.ndarray) -> float:
    try:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    except Exception:
        return 0.0
    return float(cv2.Laplacian(gray, cv2.CV_64F).var())


# -------------------------------
# Face embeddings extraction
# -------------------------------

def extract_embeddings(
    image_paths: List[Path],
    providers: List[str] = ("CPUExecutionProvider",),
    det_size=(640, 640),
    min_score: float = MIN_DET_SCORE,
    min_blur_var: float = MIN_BLUR_VAR,
    progress_callback=None,
) -> Tuple[np.ndarray, List[Path], Dict[Path, int], List[Path], List[Path]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (X, owners, img_face_count, unreadable, no_faces)
      - X: np.ndarray [N, D] L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
      - owners: list[Path] –¥–ª–∏–Ω—ã N (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ ‚Üí –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª)
      - img_face_count: dict[Path] ‚Üí –∫–æ–ª-–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ª–∏—Ü –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
      - unreadable: —Å–ø–∏—Å–æ–∫ –±–∏—Ç—ã—Ö/–Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
      - no_faces: —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –ø—Ä–∏–≥–æ–¥–Ω—ã—Ö –ª–∏—Ü
    """
    if not _INSIGHTFACE_OK or FaceAnalysis is None:
        if progress_callback:
            progress_callback("‚ùå InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install insightface", 0)
        raise RuntimeError("InsightFace –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install insightface")
    
    try:
        app = FaceAnalysis(name="buffalo_l", providers=list(providers))
        ctx_id = -1 if "cpu" in str(providers).lower() else 0
        app.prepare(ctx_id=ctx_id, det_size=det_size)
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InsightFace: {str(e)}", 0)
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InsightFace: {str(e)}")

    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...", 10)

    embeddings: List[np.ndarray] = []
    owners: List[Path] = []
    img_face_count: Dict[Path, int] = {}
    unreadable: List[Path] = []
    no_faces: List[Path] = []

    total = len(image_paths)

    for i, p in enumerate(image_paths):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total, 1) * 60)  # 10-70
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {percent}% ({i+1}/{total}) - {p.name}", percent)

        img = imread_safe(p)
        if img is None:
            unreadable.append(p)
            continue

        faces = app.get(img)
        if not faces:
            no_faces.append(p)
            continue

        used = 0
        for f in faces:
            if getattr(f, "det_score", 1.0) < min_score:
                continue
            # Blur check –Ω–∞ –∫—Ä–æ–ø–µ bbox (–≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞)
            try:
                x1, y1, x2, y2 = map(int, f.bbox)
                x1 = max(0, x1); y1 = max(0, y1); x2 = min(img.shape[1], x2); y2 = min(img.shape[0], y2)
                crop = img[y1:y2, x1:x2]
                if crop.size == 0:
                    continue
                if _blur_var(crop) < min_blur_var:
                    continue
            except Exception:
                # –µ—Å–ª–∏ bbox —Å—Ç—Ä–∞–Ω–Ω—ã–π ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue

            emb = getattr(f, "normed_embedding", None)
            if emb is None:
                continue
            emb = emb.astype(np.float32)
            # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            n = np.linalg.norm(emb)
            if n <= 0:
                continue
            emb = emb / n

            embeddings.append(emb)
            owners.append(p)
            used += 1

        if used > 0:
            img_face_count[p] = used
        else:
            no_faces.append(p)

    if not embeddings:
        return np.empty((0, 512), dtype=np.float32), owners, img_face_count, unreadable, no_faces

    X = np.vstack(embeddings).astype(np.float32)
    # –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –¥–ª—è FAISS dot==cos
    if _FAISS_OK:
        try:
            faiss.normalize_L2(X)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ FAISS –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    return X, owners, img_face_count, unreadable, no_faces


# -------------------------------
# kNN Graph + Mutual Edges + Components
# -------------------------------

def _knn_faiss(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
    d = X.shape[1]
    index = faiss.IndexHNSWFlat(d, 32)
    index.hnsw.efConstruction = 200
    index.add(X)
    index.hnsw.efSearch = 128
    D, I = index.search(X, k)
    # D ‚Äî squared L2 –ø—Ä–∏ IndexHNSWFlat; –Ω–æ –ø–æ—Å–ª–µ normalize_L2 dot –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∫–∞–∫ 2-0.5*D? –ù–µ—Ç. –î–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ–º sim –∫–∞–∫ dot.
    # –ü–µ—Ä–µ—Å—á—ë—Ç sim –±—É–¥–µ–º –¥–µ–ª–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º I.
    return D, I


def _knn_sklearn_cosine(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:
    # cosine distance ‚Üí 1 - cosine similarity
    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='cosine')
    nbrs.fit(X)
    distances, indices = nbrs.kneighbors(X)
    # –ü—Ä–µ–≤—Ä–∞—Ç–∏–º –≤ similarity
    sims = 1.0 - distances
    return sims, indices


def build_mutual_edges(
    X: np.ndarray,
    k: int = KNN_K,
    t_strict: Optional[float] = None,  # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
    mutual_rank: int = MUTUAL_RANK,
    min_shared_neighbors: int = MIN_SHARED_NEIGHBORS,
) -> List[Tuple[int, int, float]]:
    """–°—Ç—Ä–æ–∏–º —Å–ø–∏—Å–æ–∫ –Ω–∞–¥—ë–∂–Ω—ã—Ö —Ä—ë–±–µ—Ä —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º.
    1) kNN (FAISS/Sklearn)
    2) –ª–æ–∫–∞–ª—å–Ω–∞—è robust z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–æ–±–∏—è
    3) —É—Å–∏–ª–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é (Jaccard –æ–±—â–∏—Ö —Å–æ—Å–µ–¥–µ–π)
    4) –∞–≤—Ç–æ–ø–æ—Ä–æ–≥ –ø–æ —Ü–µ–ª–µ–≤–æ–π —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–µ–ø–µ–Ω–∏ DEGREE_TARGET
    """
    N = X.shape[0]
    k = min(k, max(2, N))

    # 1) kNN
    if _FAISS_OK and N >= 1000:
        _, I = _knn_faiss(X, k)
        sims = np.zeros((N, k), dtype=np.float32)
        for i in range(N):
            sims[i] = np.dot(X[I[i]].astype(np.float32), X[i].astype(np.float32))
    else:
        sims, I = _knn_sklearn_cosine(X, k)

    # 2) –ª–æ–∫–∞–ª—å–Ω–∞—è robust z-–Ω–æ—Ä–º–∞ (–±–µ–∑ self-—Å—Ç–æ–ª–±—Ü–∞)
    neigh = sims[:, 1:]
    med = np.median(neigh, axis=1, keepdims=True)
    mad = np.median(np.abs(neigh - med), axis=1, keepdims=True)
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –¥–ª—è z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    # med_full –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç—É –∂–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —á—Ç–æ –∏ sims
    med_full = np.concatenate([np.zeros((N,1),dtype=np.float32), np.tile(med, (1, k-1))], axis=1)
    mad_full = np.concatenate([np.ones((N,1),dtype=np.float32), np.tile(mad, (1, k-1))], axis=1)
    z = (sims - med_full) / (1.4826*(mad_full+1e-6))

    neighbor_sets = [set(I[i, 1:]) for i in range(N)]
    rank = [{int(I[i, r]): r for r in range(1, I.shape[1])} for i in range(N)]

    # 3) –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º z –∏ Jaccard
    cand = []  # (i, j, w)
    for i in range(N):
        for jpos, j in enumerate(I[i, 1:]):
            if j < 0 or j == i:
                continue
            # –≤–∑–∞–∏–º–Ω—ã–π TOP-R
            ri = rank[i].get(j, 10**9)
            rj = rank[j].get(i, 10**9)
            if ri > mutual_rank or rj > mutual_rank:
                continue
            # –æ–±—â–∏–µ —Å–æ—Å–µ–¥–∏
            inter = len(neighbor_sets[i].intersection(neighbor_sets[j]))
            if inter < min_shared_neighbors:
                continue
            # —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π z
            zj_i = float(z[i, jpos+1])
            j_rank = rank[j].get(i, None)
            zj_j = float(z[j, j_rank]) if j_rank is not None else 0.0
            zsym = 0.5*(zj_i + zj_j)
            # Jaccard
            jac = inter / max(1, len(neighbor_sets[i].union(neighbor_sets[j])))
            w = zsym * (0.5 + 0.5*jac)
            cand.append((i, j, w))

    if not cand:
        return []

    # –°–∏–º–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—è –ø–∞—Ä (i<j)
    pair = {}
    for i, j, w in cand:
        a, b = (i, j) if i < j else (j, i)
        pair.setdefault((a, b), []).append(w)
    scores = np.array([np.mean(ws) for ws in pair.values()], dtype=np.float32)

    # 4) –∞–≤—Ç–æ–ø–æ—Ä–æ–≥ –ø–æ DEGREE_TARGET
    def avg_degree(th):
        m = 0
        for w in scores:
            if w >= th:
                m += 1
        return (2.0*m) / max(1, N)

    low, high = np.percentile(scores, 10), np.percentile(scores, 90)
    target_lo, target_hi = DEGREE_TARGET
    t = (low + high) / 2.0
    for _ in range(20):
        deg = avg_degree(t)
        if deg < target_lo:
            high = t
        elif deg > target_hi:
            low = t
        else:
            break
        t = (low + high) / 2.0

    edges = []
    idx = 0
    for (a, b), wlist in pair.items():
        score = float(scores[idx]); idx += 1
        if score >= t:
            edges.append((a, b, score))
    return edges


def connected_components_from_edges(N: int, edges: List[Tuple[int, int, float]]) -> List[List[int]]:
    if _NX_OK:
        G = nx.Graph()
        G.add_nodes_from(range(N))
        for u, v, w in edges:
            G.add_edge(u, v, weight=w)
        return [list(c) for c in nx.connected_components(G)]
    # Fallback: union-find (–±—ã—Å—Ç—Ä–æ –∏ –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
    parent = list(range(N))

    def find(x):
        while parent[x] != x:
            parent[x] = parent[parent[x]]
            x = parent[x]
        return x

    def union(a, b):
        ra, rb = find(a), find(b)
        if ra != rb:
            parent[rb] = ra

    for u, v, _ in edges:
        union(u, v)

    groups = defaultdict(list)
    for i in range(N):
        groups[find(i)].append(i)
    return list(groups.values())


# -------------------------------
# Medoid membership filtering & optional inter-cluster merge
# -------------------------------

def _medoid_index(points: np.ndarray) -> int:
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—É–º–º–æ–π –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Å—Ö–æ–¥—Å—Ç–≤ –∫ –¥—Ä—É–≥–∏–º
    # —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ argmax –ø–æ —Å—É–º–º–µ dot, —Ç–∞–∫ –∫–∞–∫ X —É–∂–µ L2-–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω
    S = np.dot(points, points.T)
    sums = S.sum(axis=1)
    return int(np.argmax(sums))


def filter_by_medoid(
    X: np.ndarray,
    components: List[List[int]],
    t_member: float = T_MEMBER,
) -> List[List[int]]:
    filtered = []
    for comp in components:
        if len(comp) == 1:
            filtered.append(comp)
            continue
        P = X[comp]
        m_idx_local = _medoid_index(P)
        m = P[m_idx_local]
        sims = np.dot(P, m)
        keep = [comp[i] for i, s in enumerate(sims) if float(s) >= t_member]
        if len(keep) >= 1:
            filtered.append(keep)
    return filtered


def optional_merge_by_centroids(
    X: np.ndarray,
    clusters: List[List[int]],
    t_merge: float = T_MERGE,
    cluster_attrs: Optional[List[dict]] = None,
) -> List[List[int]]:
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –º–∞–∫—Å–∏–º—É–º–∞ precision —ç—Ç–æ—Ç —ç—Ç–∞–ø –Ω–µ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —É—Ç–∏–ª–∏—Ç—É)
    C = [np.mean(X[c], axis=0) for c in clusters]
    C = [c / (np.linalg.norm(c) + 1e-12) for c in C]

    merged = []
    used = [False] * len(clusters)

    def attrs_conflict(a, b) -> bool:
        if not cluster_attrs:
            return False
        A, B = cluster_attrs[a], cluster_attrs[b]
        # –ü—Ä–æ—Å—Ç–µ–π—à–∏–µ –≥–µ–π—Ç—ã: –µ—Å–ª–∏ –æ–±–∞ –∑–Ω–∞—é—Ç gender –∏ –æ–Ω–∏ —Ä–∞–∑–Ω—ã–µ ‚Äî –∫–æ–Ω—Ñ–ª–∏–∫—Ç
        ga, gb = A.get('gender'), B.get('gender')
        if ga is not None and gb is not None and ga != gb:
            return True
        # –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –±–∏–Ω—ã: –µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è —Å–∏–ª—å–Ω–æ ‚Äî –±–ª–æ–∫
        ya, yb = A.get('age'), B.get('age')
        if ya is not None and yb is not None and abs(ya - yb) >= 15:
            return True
        return False

    for i in range(len(clusters)):
        if used[i]:
            continue
        cur = list(clusters[i])
        used[i] = True
        for j in range(i + 1, len(clusters)):
            if used[j]:
                continue
            sim = float(np.dot(C[i], C[j]))
            if sim >= t_merge and not attrs_conflict(i, j):
                cur.extend(clusters[j])
                used[j] = True
        merged.append(sorted(cur))
    return merged


# -------------------------------
# High-Precision clustering (end-to-end)
# -------------------------------

def hi_precision_cluster(
    X: np.ndarray,
    t_member: float = T_MEMBER,
    allow_merge: bool = False,
    t_merge: float = T_MERGE,
    progress_callback=None,
) -> List[List[int]]:
    N = X.shape[0]
    if N == 0:
        return []

    if progress_callback:
        progress_callback("üîó –°—Ç—Ä–æ–∏–º kNN-–≥—Ä–∞—Ñ (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥)...", 75)
    edges = build_mutual_edges(X, k=KNN_K)

    if progress_callback:
        progress_callback("üß© –°–≤—è–∑–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã...", 82)
    comps = connected_components_from_edges(N, edges)

    if progress_callback:
        progress_callback("üéØ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ–¥–∏–æ–∏–¥—É...", 88)
    clusters = filter_by_medoid(X, comps, t_member=t_member)

    if allow_merge:
        if progress_callback:
            progress_callback("üß¨ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –ø–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º...", 92)
        clusters = optional_merge_by_centroids(X, clusters, t_merge=t_merge, cluster_attrs=None)

    return clusters

# -------------------------------
# Public API: build_plan_live (Hi-Precision)
# -------------------------------

def build_plan_live(
    input_dir: Path,
    det_size=(640, 640),
    min_score: float = MIN_DET_SCORE,
    min_cluster_size: int = 2,  # not used in graph flow; –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã
    min_samples: int = 2,       # not used
    providers: List[str] = ("CPUExecutionProvider",),
    progress_callback=None,
    include_excluded: bool = False,
    allow_merge: bool = False,
    t_strict: float = T_STRICT,
    t_member: float = T_MEMBER,
    t_merge: float = T_MERGE,
):
    """
    Hi-Precision –≤–µ—Ä—Å–∏—è build_plan_live: –±–µ–∑ O(N^2), —Å –≥—Ä–∞—Ñ–æ–≤–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π –∏ –º–µ–¥–∏–æ–∏–¥–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏: clusters, plan, unreadable, no_faces.
    """
    input_dir = Path(input_dir)
    if include_excluded:
        all_images = [p for p in input_dir.rglob('*') if is_image(p)]
    else:
        all_images = [p for p in input_dir.rglob('*') if is_image(p) and not any(ex in str(p).lower() for ex in EXCLUDED_NAMES)]

    if progress_callback:
        progress_callback(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è: {input_dir}, –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 1)

    # 1) Embeddings
    try:
        X, owners, img_face_count, unreadable, no_faces = extract_embeddings(
            all_images,
            providers=providers,
            det_size=det_size,
            min_score=min_score,
            progress_callback=progress_callback,
        )
    except Exception as e:
        if progress_callback:
            progress_callback(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}", 100)
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in all_images],
            "no_faces": [],
            "error": str(e)
        }

    if X.shape[0] == 0:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", 100)
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    # 2) Graph clustering
    if progress_callback:
        progress_callback(f"üîÑ –ì—Ä–∞—Ñ–æ–≤–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {X.shape[0]} –ª–∏—Ü...", 80)
    clusters_idx = hi_precision_cluster(
        X,
        t_member=t_member,
        allow_merge=allow_merge,
        t_merge=t_merge,
        progress_callback=progress_callback,
    )

    # 3) –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—ã
    cluster_map: Dict[int, Set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)
    for new_label, comp in enumerate(clusters_idx):
        for idx in comp:
            p = owners[idx]
            cluster_map[new_label].add(p)
            cluster_by_img[p].add(new_label)

    # 4) –ü–ª–∞–Ω —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    if progress_callback:
        progress_callback("üì¶ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...", 95)
    plan = []
    # –û–±—Ö–æ–¥–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≤—ã–±–æ—Ä–∫–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫ –∏ —É—á–µ—Å—Ç—å multi-face
    seen = set()
    for p in owners:
        if p in seen:
            continue
        seen.add(p)
        clusters = sorted(list(cluster_by_img.get(p, set())))
        if clusters:
            plan.append({
                "path": str(p),
                "cluster": clusters,
                "faces": img_face_count.get(p, 0)
            })

    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(plan)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 100)

    return {
        "clusters": {int(k): [str(x) for x in sorted(v, key=lambda s: str(s))] for k, v in cluster_map.items()},
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }


# -------------------------------
# Distribution (—Ñ–∏–∫—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ª–æ–≥–∏–∫–∏)
# -------------------------------

def distribute_to_folders(
    plan: dict,
    base_dir: Path,
    cluster_start: int = 1,
    progress_callback=None,
    keep_original_on_multi: bool = True,
    annotate_folders: bool = False
) -> Tuple[int, int, int]:
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)

    cluster_file_counts: Dict[int, int] = {}
    for item in plan_items:
        for cluster_id in (cluster_id_map[c] for c in item["cluster"]):
            cluster_file_counts[cluster_id] = cluster_file_counts.get(cluster_id, 0) + 1

    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)

        src = Path(item["path"])
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue

        if len(clusters) == 1:
            cluster_id = clusters[0]
            dst = base_dir / f"{cluster_id}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                try:
                    if src.resolve() == dst.resolve():
                        continue
                except Exception:
                    if str(src) == str(dst):
                        continue
                _safe_move(src, dst)
                moved += 1
                moved_paths.add(src.parent)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {src} ‚Üí {dst}: {e}")
        else:
            # multi: –∫–æ–ø–∏—Ä—É–µ–º –≤ –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä, –æ—Ä–∏–≥–∏–Ω–∞–ª –æ—Å—Ç–∞–≤–ª—è–µ–º
            for cluster_id in clusters:
                dst = base_dir / f"{cluster_id}" / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    try:
                        if src.resolve() == dst.resolve():
                            continue
                    except Exception:
                        if str(src) == str(dst):
                            continue
                    _safe_copy(src, dst)
                    copied += 1
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src} ‚Üí {dst}: {e}")

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–µ–ª–∞–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî Off)
    if annotate_folders and cluster_file_counts:
        if progress_callback:
            progress_callback("üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–∞–ø–æ–∫...", 95)
        for cluster_id, file_count in cluster_file_counts.items():
            old_folder = base_dir / str(cluster_id)
            new_folder = base_dir / f"{cluster_id} ({file_count})"
            if old_folder.exists() and old_folder.is_dir():
                try:
                    old_folder.rename(new_folder)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è {old_folder} ‚Üí {new_folder}: {e}")

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)
    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            if p.exists() and not any(p.iterdir()):
                p.rmdir()
        except Exception:
            pass

    return moved, copied, cluster_start + len(used_clusters)


# -------------------------------
# "–û–±—â–∏–µ" –ø–∞–ø–∫–∏: –ø–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
# -------------------------------

def find_common_folders_recursive(root_dir: Path, max_depth: int = 3):
    common_folders = []

    def scan_directory(dir_path: Path, level=0):
        if level > max_depth:
            return
        try:
            for item in dir_path.iterdir():
                if item.is_dir():
                    low = item.name.lower()
                    if any(ex in low for ex in EXCLUDED_NAMES):
                        common_folders.append(item)
                    else:
                        scan_directory(item, level + 1)
        except PermissionError:
            print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–ø–∫–µ: {dir_path}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {dir_path}: {e}")

    scan_directory(Path(root_dir))
    return common_folders


def process_common_folder_at_level(common_dir: Path, progress_callback=None) -> int:
    parent_dir = common_dir.parent
    existing_ids = _collect_existing_numeric_ids(parent_dir)

    data = build_plan_live(common_dir, include_excluded=True, progress_callback=progress_callback)
    plan = data.get('plan', [])
    if not plan:
        return 0

    # –ú–∞–ø–ø–∏–Ω–≥: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–º –¥–∞—ë–º –Ω–æ–≤—ã–µ ID –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    used_clusters_src = sorted({c for item in plan for c in item['cluster']})
    next_id = (max(existing_ids) + 1) if existing_ids else 1
    cluster_id_map = {}
    for old in used_clusters_src:
        if old in existing_ids:
            cluster_id_map[old] = old
        else:
            cluster_id_map[old] = next_id
            next_id += 1

    remapped_plan = []
    for item in plan:
        remapped_plan.append({
            'path': item['path'],
            'cluster': sorted(cluster_id_map[c] for c in item['cluster']),
            'faces': item.get('faces', 0)
        })

    moved, copied, _ = distribute_to_folders(
        {"plan": remapped_plan},
        base_dir=parent_dir,
        cluster_start=1,
        progress_callback=progress_callback,
        keep_original_on_multi=True,
        annotate_folders=False,
    )
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–ª-–≤–æ –∫–æ–ø–∏–π –∫–∞–∫ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ä–∞–±–æ—Ç—ã
    return copied


def process_group_folder(group_dir: Path, progress_callback=None, include_excluded: bool = False):
    cluster_counter = 1
    group_dir = Path(group_dir)

    if include_excluded:
        if progress_callback:
            progress_callback("üîç –ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ' –≤–æ –≤—Å–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏...", 10)
        common_folders = find_common_folders_recursive(group_dir)
        if not common_folders:
            if progress_callback:
                progress_callback("‚ùå –ü–∞–ø–∫–∏ '–æ–±—â–∏–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã", 100)
            return 0, 0, cluster_counter

        total_copied = 0
        total = len(common_folders)
        for i, common_folder in enumerate(common_folders):
            if progress_callback:
                percent = 20 + int((i + 1) / total * 70)
                progress_callback(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è: {common_folder.name} ({i+1}/{total})", percent)
            total_copied += process_common_folder_at_level(common_folder, progress_callback)

        if progress_callback:
            progress_callback(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ '–æ–±—â–∏–µ': {total_copied} —Ñ–∞–π–ª–æ–≤", 100)
        return 0, total_copied, cluster_counter

    # –ò–Ω–∞—á–µ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ, –∏—Å–∫–ª—é—á–∞—è '–æ–±—â–∏–µ'
    subfolders = [f for f in sorted(group_dir.iterdir()) if f.is_dir() and "–æ–±—â–∏–µ" not in f.name.lower()]
    total_subfolders = len(subfolders)

    for i, subfolder in enumerate(subfolders):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total_subfolders, 1) * 80)
            progress_callback(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ({i+1}/{total_subfolders})", percent)

        plan = build_plan_live(subfolder, progress_callback=progress_callback)
        moved, copied, cluster_counter = distribute_to_folders(
            plan, subfolder, cluster_start=cluster_counter, progress_callback=progress_callback
        )

    return 0, 0, cluster_counter
